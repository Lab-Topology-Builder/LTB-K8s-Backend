{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lab Topology Builder Documentation","text":"<p>The Lab Topology Builder (LTB) is an open source project that allows users to deploy networking labs on Kubernetes. It is a tool that enables you to build a topology of virtual machines and containers, which are connected to each other according to the network topology you have defined.</p> <p>To get started, please refer to the User Guide.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Deployment of networking labs on Kubernetes</li> <li>Deletion of networking labs on Kubernetes</li> <li>Status querying of lab deployments</li> <li>Remote access to lab nodes' console via web browser</li> <li>Remote access to lab nodes' OOB management (e.g. SSH)</li> <li>Managing custom node types</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"about/","title":"About","text":"<p>This project was created as part of a bachelor thesis at the Eastern Switzerland University of Applied Sciences in Rapperswil, in cooperation with the Institue for Network and Security.</p>"},{"location":"about/#authors","title":"Authors","text":"<ul> <li>Jan Untersander</li> <li>Tsigereda Nebai Kidane</li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#lab-topology-builder-ltb","title":"Lab Topology Builder (LTB)","text":"<p>The Lab Topology Builder (LTB) is a tool that allows you to build a topology of virtual machines and containers, which are connected to each other according to the network topology you have defined.</p> <p></p>"},{"location":"concepts/#ltb-kubernetes-backend","title":"LTB Kubernetes Backend","text":"<p>The LTB Kubernetes Backend consists of the LTB Operator and the LTB Operator API, which replaces the current (KVM/Docker) based backend of LTB. It is responsible for managing the lab instances in a Kubernetes cluster and provide a REST API for the frontend to interact with the kubernetes cluster.</p>"},{"location":"concepts/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>Kubernetes Cluster is a set of nodes that run containerized applications managed by Kubernetes. A Kubernetes cluster consists of a control plane and one or more nodes. The control plane is responsible for maintaining the desired state of the cluster, such as which applications are running and which container images they use, etc. And the nodes run the applications and workloads.</p>"},{"location":"concepts/#kubernetes-operator","title":"Kubernetes operator","text":"<p>A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts but includes domain or application-specific knowledge to automate the entire life cycle of the software it manages.</p>"},{"location":"concepts/#ltb-kubernetes-operator","title":"LTB Kubernetes Operator","text":"<p>The LTB Kubernetes Operator is a K8s Operator for the LTB application, which manages lab templates, lab instances in a Kubernetes cluster. It manages lab instances, node types, etc. It also updates the status of the lab instances according to the current state of the containers and virtual machines that are part of the lab instance.</p>"},{"location":"concepts/#custom-resource-definition-crd","title":"Custom Resource Definition (CRD)","text":"<p>A custom resource definition (CRD) is a Kubernetes native resource. Defining a CRD object creates a new custom resource with a name and schema that you specify. The custom resource created from a CRD object can be either namespaced or cluster-scoped. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.</p> <p>The following example shows a CRD for a LabInstance object, how they are defined in go and how the YAML output looks like.</p> <pre><code>import (\nmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\ntype LabInstanceSpec struct {\nLabTemplateReference string `json:\"labTemplateReference\"`\n}\n\ntype LabInstanceStatus struct {\nStatus         string `json:\"status,omitempty\"`\nNumPodsRunning string `json:\"numpodsrunning,omitempty\"`\nNumVMsRunning  string `json:\"numvmsrunning,omitempty\"`\n}\n\n//+kubebuilder:object:root=true\n//+kubebuilder:subresource:status\n//+kubebuilder:printcolumn:name=\"STATUS\",type=string,JSONPath=`.status.status`\n//+kubebuilder:printcolumn:name=\"PODS_RUNNING\",type=string,JSONPath=`.status.numpodsrunning`\n//+kubebuilder:printcolumn:name=\"VMS_RUNNING\",type=string,JSONPath=`.status.numvmsrunning`\n\ntype LabInstance struct {\nmetav1.TypeMeta   `json:\",inline\"`\nmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\nSpec   LabInstanceSpec   `json:\"spec,omitempty\"`\nStatus LabInstanceStatus `json:\"status,omitempty\"`\n}\n\n// +kubebuilder:object:root=true\ntype LabInstanceList struct {\nmetav1.TypeMeta `json:\",inline\"`\nmetav1.ListMeta `json:\"metadata,omitempty\"`\nItems           []LabInstance `json:\"items\"`\n}\n</code></pre> <pre><code>---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\nannotations:\ncontroller-gen.kubebuilder.io/version: v0.10.0\ncreationTimestamp: null\nname: labinstances.ltb-backend.ltb\nspec:\ngroup: ltb-backend.ltb\nnames:\nkind: LabInstance\nlistKind: LabInstanceList\nplural: labinstances\nsingular: labinstance\nscope: Namespaced\nversions:\n- additionalPrinterColumns:\n- jsonPath: .status.status\nname: STATUS\ntype: string\n- jsonPath: .status.numpodsrunning\nname: PODS_RUNNING\ntype: string\n- jsonPath: .status.numvmsrunning\nname: VMS_RUNNING\ntype: string\nname: v1alpha1\nschema:\nopenAPIV3Schema:\nproperties:\napiVersion:\ndescription: 'APIVersion defines the versioned schema of this representation\nof an object. Servers should convert recognized schemas to the latest\ninternal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\ntype: string\nkind:\ndescription: 'Kind is a string value representing the REST resource this\nobject represents. Servers may infer this from the endpoint the client\nsubmits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\ntype: string\nmetadata:\ntype: object\nspec:\nproperties:\nlabTemplateReference:\ntype: string\nrequired:\n- labTemplateReference\ntype: object\nstatus:\nproperties:\nnumpodsrunning:\ntype: string\nnumvmsrunning:\ntype: string\nstatus:\ntype: string\ntype: object\ntype: object\nserved: true\nstorage: true\nsubresources:\nstatus: {}\n</code></pre>"},{"location":"concepts/#custom-resource-cr","title":"Custom Resource (CR)","text":"<p>A custom resource (CR) is an extension of the Kubernetes API that allows you to define and manage your own API objects. It provides a way to store and retrieve structured data and can be used with a custom controller to provide a declarative API. Custom resources can be defined as a Kubernetes API extension using Custom Resource Definitions (CRDs) or via API aggregation.</p>"},{"location":"concepts/#lab-template","title":"Lab Template","text":"<p>Lab Template is a CR, which defines a template for a lab. It contains information about the lab you want to deploy and a reference to the node types that are part of the lab.</p> <p>The following YAML is an example of a LabTemplate:</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabTemplate\nmetadata:\nlabels:\napp.kubernetes.io/name: labtemplate\napp.kubernetes.io/instance: labtemplate-sample\napp.kubernetes.io/part-of: operator\napp.kubernetes.io/managed-by: kustomize\napp.kubernetes.io/created-by: operator\nname: labtemplate-sample\nspec:\nnodes:\n- name: \"sample-node-1\"\nimage:\ntype: \"ubuntu\"\nversion: \"22.04\"\nkind: \"vm\"\nconfig: |-\n#cloud-config\npassword: ubuntu\nchpasswd: { expire: False }\nssh_authorized_keys:\n- &lt;your-ssh-pub-key&gt;\npackages:\n- qemu-guest-agent\nruncmd:\n- [ systemctl, start, qemu-guest-agent ]\n- name: \"sample-node-2\"\nimage:\ntype: \"ghcr.io/insrapperswil/network-ninja\"\nversion: \"latest\"\n- name: \"sample-node-3\"\nimage:\ntype: \"ubuntu\"\nversion: \"latest\"\nkind: \"pod\"\nconnections:\n- neighbors: \"TestHost1:1,TestHost2:1\"\nport: 22\n</code></pre>"},{"location":"concepts/#lab-instance","title":"Lab Instance","text":"<p>A lab instance is a CR that describes a lab that you want to deploy in a Kubernetes cluster. It has a reference to the lab template you want to use and also has a status field that is updated by the operator, which shows how many pods and VMs are running in the lab and the status of the lab instance itself.</p> <p>The following YAML is an example of a LabInstance:</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabInstance\nmetadata:\nlabels:\napp.kubernetes.io/name: labinstance\napp.kubernetes.io/instance: labinstance-sample\napp.kubernetes.io/part-of: operator\napp.kubernetes.io/managed-by: kustomize\napp.kubernetes.io/created-by: operator\nname: labinstance-sample\nspec:\nlabTemplateReference: \"labtemplate-sample\"\n</code></pre>"},{"location":"concepts/#lab","title":"Lab","text":"<p>A deployment of a lab instance is called a lab.</p>"},{"location":"concepts/#nodetype","title":"NodeType","text":"<p>In a network, a node represents any device that is part of the lab. A NodeType is a CR that defines a type of node that can be part of a lab. You reference the node type you want to have in your lab in the lab template. Within LTB, a node can be either a KubeVirt virtual machine or a pod.</p>"},{"location":"concepts/#network-topology","title":"Network Topology","text":"<p>The arrangement or pattern in which all nodes on a network are connected together is referred to as the network\u2019s topology.</p>"},{"location":"user-guide/","title":"User Guide","text":""},{"location":"user-guide/#installation-pre-requisites","title":"Installation Pre-requisites","text":"Tool Version Installation Description Kubernetes ^1.26.0 Installation Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubevirt 0.59.0 Installation Kubevirt is a Kubernetes add-on to run virtual machines on Kubernetes. Multus-CNI 3.9.0 Installation Multus-CNI is a plugin for K8s to attach multiple network interfaces to pods. Operator Lifecycle Manager ^0.24.0 Installation Operator Lifecycle Manager (OLM) helps users install, update, and manage the lifecycle of all Operators and their associated services running across their Kubernetes clusters. <p>Alternative OLM installation:</p> <pre><code>curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.25.0/install.sh | bash -s v0.25.0\n</code></pre> <p>Change the version to the desired one.</p>"},{"location":"user-guide/#installation-of-the-ltb-k8s-operator","title":"Installation of the LTB K8s operator","text":"<ol> <li> <p>Install the operator by creating a catalog source and subscription. <pre><code>kubectl apply -f https://raw.githubusercontent.com/Lab-Topology-Builder/LTB-K8s-Backend/main/install/catalogsource.yaml -f https://raw.githubusercontent.com/Lab-Topology-Builder/LTB-K8s-Backend/main/install/subscription.yaml\n</code></pre></p> </li> <li> <p>Wait for the operator to be installed <pre><code>kubectl get csv -n operators -w\n</code></pre></p> </li> </ol>"},{"location":"user-guide/#usage","title":"Usage","text":"<p>To create a lab you'll need to create at least one node type and one lab template. Node types define the basic properties of a node. For VMs this includes everything that can be defined in a Kubevirt VirtualMachineSpec and for pods everything that can be defined in a Kubernetes PodSpec.</p> <p>In order to provide better reusability of node types, you can use Go templating Syntax to include information from the lab template (like configuration or node name) in the node type. The following example node types show how this can be done. You can use them as a starting point for your own node types.</p>"},{"location":"user-guide/#example-node-type","title":"Example Node Type","text":"<p>This is an example of a VM node type. It creates a VM with 2 vCPUs and 4GB of RAM, using the Ubuntu 22.04 container disk image from quay.io/containerdisks/ubuntu and the <code>cloudInitNoCloud</code> volume source to provide a cloud-init configuration to the VM.</p> <p>Everything that is defined in the <code>node</code> field of the lab template is available to the node type via the <code>.</code> variable. Example: <code>{{ .Name }}</code> will be replaced with the name of the node from the lab template.</p> <p>Currently, you cannot provide the cloud-init configuration as a YAML string via the .Config field of the lab template. Instead, you have to encode it as base64 string and therefore use the <code>userDataBase64</code> field of the volume source, because of indentation issues while rendering configuration.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: NodeType\nmetadata:\nname: nodetypeubuntuvm\nspec:\nkind: vm\nnodeSpec: |\nrunning: true\ntemplate:\nspec:\ndomain:\nresources:\nrequests:\nmemory: 4096M\ncpu:\ncores: 2\ndevices:\ndisks:\n- name: containerdisk\ndisk:\nbus: virtio\n- name: cloudinitdisk\ndisk:\nbus: virtio\nterminationGracePeriodSeconds: 0\nvolumes:\n- name: containerdisk\ncontainerDisk:\nimage: quay.io/containerdisks/ubuntu:22.04\n- name: cloudinitdisk\ncloudInitNoCloud:\nuserDataBase64: {{ .Config }}\n</code></pre> <p>This is an example of a generic pod node type. It creates a pod with a single container. The container name, container image, command and ports to expose are taken from the lab template.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: NodeType\nmetadata:\nname: genericpod\nspec:\nkind: pod\nnodeSpec: |\ncontainers:\n- name: {{ .Name }}\nimage: {{ .NodeTypeRef.Image}}:{{ .NodeTypeRef.Version }}\ncommand: {{ .Config }}\nports:\n{{- range $index, $port := .Ports }}\n- name: {{ $port.Name }}\ncontainerPort: {{ $port.Port }}\nprotocol: {{ $port.Protocol }}\n{{- end }}\n</code></pre> <p>After you have defined some node types, you can create a lab template. A lab template defines the nodes that should be created for a lab, how they should be configured and how they should be connected.</p>"},{"location":"user-guide/#example-lab-template","title":"Example Lab Template","text":"<p>This is an example of a lab template, which you can use as a starting point for your own labs. It uses the previously defined node types to create a VM and two pods. They are referenced via the <code>nodeTypeRef</code> field. The provided ports will be exposed to the host network and can be accessed via the node's IP address and the port number assigned by Kubernetes. You can retrieve the IP address of a node by running <code>kubectl get node -o wide</code> and the port number by running <code>kubectl get svc</code>.</p> <p>Currently, there is no support for point to point connections between nodes. Instead, they are all connected to the same network.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabTemplate\nmetadata:\nname: labtemplate-sample\nspec:\nnodes:\n- name: \"sample-node-1\"\nnodeTypeRef:\ntype: \"nodetypeubuntuvm\"\nconfig: \"I2Nsb3VkLWNvbmZpZwpwYXNzd29yZDogdWJ1bnR1CmNocGFzc3dkOiB7IGV4cGlyZTogRmFsc2UgfQpzc2hfcHdhdXRoOiBUcnVlCnBhY2thZ2VzOgogLSBxZW11LWd1ZXN0LWFnZW50CiAtIGNtYXRyaXgKcnVuY21kOgogLSBbIHN5c3RlbWN0bCwgc3RhcnQsIHFlbXUtZ3Vlc3QtYWdlbnQgXQo=\"\nports:\n- name: \"ssh\"\nport: 22\nprotocol: \"TCP\"\n- name: \"sample-node-2\"\nnodeTypeRef:\ntype: \"genericpod\"\nimage: \"ghcr.io/insrapperswil/network-ninja\"\nversion: \"latest\"\nports:\n- name: \"ssh\"\nport: 22\nprotocol: \"TCP\"\nconfig: '[\"/bin/bash\", \"-c\", \"apt update &amp;&amp; apt install -y openssh-server &amp;&amp; service ssh start &amp;&amp; sleep 365d\"]'\n- name: \"sample-node-3\"\nnodeTypeRef:\ntype: \"genericpod\"\nimage: \"ubuntu\"\nversion: \"22.04\"\nports:\n- name: \"ssh\"\nport: 22\nprotocol: \"TCP\"\nconfig: '[\"/bin/bash\", \"-c\", \"apt update &amp;&amp; apt install -y openssh-server &amp;&amp; service ssh start &amp;&amp; sleep 365d\"]'\n</code></pre> <p>With the lab template defined, you can create a lab instance.</p>"},{"location":"user-guide/#example-lab-instance","title":"Example Lab Instance","text":"<p>This is an example of lab instance, which you can use as a starting point for your own labs. The lab instance references the previously defined lab template with the <code>labTemplateReference</code> field. You also need to provide a DNS address via the <code>dnsAddress</code> field. This address will be used to create routes for the web terminal to the lab nodes. For example, if you use the address <code>example.com</code>, the console of a node called <code>sample-node-1</code> will be available at <code>https://labinstance-sample-sample-node-1.example.com/</code> via a web terminal.</p> <p>Currently, there is no support to edit the lab instance after it has been created. If you want to change the lab, you have to delete the lab instance and create a new one.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabInstance\nmetadata:\nname: labinstance-sample\nspec:\nlabTemplateReference: \"labtemplate-sample\"\ndnsAddress: \"example.com\"\n</code></pre>"},{"location":"user-guide/#uninstall","title":"Uninstall","text":"<ol> <li> <p>Delete the subscription <pre><code>kubectl delete subscriptions.operators.coreos.com -n operators ltb-subscription\n</code></pre></p> </li> <li> <p>Delete the CSV <pre><code>kubectl delete csv -n operators ltb-operator.&lt;version&gt;\n</code></pre></p> </li> <li> <p>Delete the CRDs <pre><code>kubectl delete crd labinstances.ltb-backend.ltb labtemplates.ltb-backend.ltb nodetypes.ltb-backend.ltb\n</code></pre></p> </li> <li> <p>Delete operator <pre><code>kubectl delete operator ltb-operator.operators\n</code></pre></p> </li> <li> <p>Delete the CatalogSource <pre><code>kubectl delete catalogsource.operators.coreos.com -n operators ltb-catalog\n</code></pre></p> </li> </ol>"},{"location":"architecture/k8s-ltb-architecture/","title":"Kubernetes Lab Topology Builder Architecture","text":"<p>The main components of the Kubernetes based LTB are:</p> <ul> <li>Frontend</li> <li>API</li> <li>Operator</li> </ul> <p>The following diagram shows how the components interact with each other:</p> <p></p>"},{"location":"architecture/k8s-ltb-architecture/#frontend","title":"Frontend","text":"<p>The frontend can be implemented in any language and framework, it just needs to be able to communicate via an HTTP API with the LTB API. The frontend is responsible for the following tasks:</p> <ul> <li>Providing a web UI for the user to interact with the labs.</li> <li>Providing a web UI for the admin to manage:</li> <li>Lab templates</li> <li>Lab deployments</li> <li>Reservations</li> </ul> <p>There is a possibility to reuse parts of the existing frontend from the KVM/Docker based LTB.</p>"},{"location":"architecture/k8s-ltb-architecture/#api","title":"API","text":"<p>The API is responsible for the following tasks:</p> <ul> <li>To create, update and delete LTB resources (node types, lab templates, lab instances)</li> <li>Exposes status of lab instances</li> <li>Expose information on how to access the deployed lab nodes</li> <li>Authentication via an external authentication provider</li> </ul> <p>No parts from the existing KVM/Docker based LTB can be reused for the API.</p>"},{"location":"architecture/k8s-ltb-architecture/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>The authentication can be implemented by using an external authentication provider like Keycloak. Keycloak can be configured to act as an authentication broker with external identity providers like LDAP, OpenID Connect, SAML, etc. This has the benefit that the LTB does not need to implement any authentication logic and can focus on the lab deployment. Additionally, it enables the LTB to be integrated into existing authentication infrastructures, with the benefit that users do not need to create a new account. On the other hand, it has the drawback that the LTB needs an external authentication provider to work and that the users access rights would need to be managed in Keycloak.</p> <p>Authorization can also be implemented using Keycloak and its Authorization Services.</p>"},{"location":"architecture/k8s-ltb-architecture/#operator","title":"Operator","text":"<p>The operator is responsible for the following tasks:</p> <ul> <li>Deploying/destroying the containers and vms</li> <li>Check validity of LTB resources (node types, lab templates, lab instances)</li> <li>Enables you to access the deployed containers and vms via different protocols</li> <li>Providing remote access to the lab node console via a web terminal</li> <li>Managing reservations (create, delete, etc.)</li> <li>Providing remote Wireshark capture capabilities</li> </ul> <p>The operator is implemented according to the Kubernetes operator pattern. It has multiple controllers that are responsible for managing a particular custom resource like a lab template.</p>"},{"location":"architecture/k8s-ltb-architecture/#network-connectivity-between-lab-nodes","title":"Network connectivity between lab nodes","text":"<p>The network connectivity between lab nodes can be implemented with Multus, which is a \"meta-plugin\" that enables attaching multiple CNI plugins to a kubernetes pod/vm. Multus uses NetworkAttachmentDefinitions (NAD) to describe, which CNI plugin should be used and how it should be configured.</p> <p>Currently we use a linux bridge as a secondary CNI plugin, with the drawback that the links between the lab nodes are not pure layer 2 links, but layer 3 links. Additionally the connection between the lab nodes only work on the same Kubernetes host, because the linux bridge does not implement any kind of cross-host networking.</p>"},{"location":"architecture/k8s-ltb-architecture/#remote-access-to-lab-nodes","title":"Remote access to lab nodes","text":"<p>Remote access to the lab nodes has two variants:</p> <ul> <li>Console access via a web terminal</li> <li>Access to configurable ports with any OOB management protocol</li> </ul>"},{"location":"architecture/k8s-ltb-architecture/#console-access-via-a-web-terminal","title":"Console access via a web terminal","text":"<p>The console access via a web terminal is implemented with kube-ttyd, which is a tool based on ttyd, with the addition to use kubectl exec and virsh console to connect to the lab nodes. <code>kube-ttyd</code> was provided by Yannick Zwicker from the INS specifically for this project. Access to the web terminal is routed through a NGINX ingress controller, and a Kubernetes service of type <code>ClusterIP</code>.</p> <p>The authentication feature of the NGINX ingress controller can be used to restrict access to the web terminal to authenticated users. It might be possible to use the same authentication provider as the LTB API, but this needs to be tested.</p>"},{"location":"architecture/k8s-ltb-architecture/#access-to-configurable-ports-with-any-oob-management-protocol","title":"Access to configurable ports with any OOB management protocol","text":"<p>Access to lab nodes via freely choosable OOB management protocols is implemented by providing a Kubernetes service of type <code>LoadBalancer</code> for each lab node, which is configured to expose the ports specified in the lab template.</p> <p>Access control needs to be implemented by the lab node itself, because the Kubernetes service of type <code>LoadBalancer</code> does not provide any authentication or authorization features. An example for this would be to provide SSH keys for the lab nodes inside the lab template config field.</p>"},{"location":"architecture/k8s-ltb-architecture/#scheduling-lab-instances-and-resource-reservation","title":"Scheduling lab instances and resource reservation","text":"<p>A feature to schedule the deployment and deletion of a lab instance to a specific time is not implemented, but could be implemented by adding an additional fields (creationTimestamp, deletionTimestamp) to the lab instance custom resource. The lab instance controller then is able to check these fields and deploy or delete the lab instance at the specified time. There are multiple ways to implement this, either by regularly checking the lab instance custom resources, or by requeuing the lab instance creation/deletion event to the specified time.</p> <p>If there are any issues with the requeuing of these events over such a long period of time, writing a Kubernetes informer could be a solution.</p> <p>Resource reservation in a capacity planning sense is not provided by Kubernetes, a manual solution could be implemented by using limit ranges, resource quotas and the Kubernetes node resources. Planned resource management is a huge topic, and we would recommend to create a dedicated project for this.</p>"},{"location":"architecture/k8s-ltb-architecture/#c4-model","title":"C4 Model","text":"<p>The following diagrams show the C4 model of the Kubernetes based LTB, it should provide a high level overview of the system.</p>"},{"location":"architecture/k8s-ltb-architecture/#system-context-diagram","title":"System Context Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#container-diagram","title":"Container Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#component-diagram","title":"Component Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#legend","title":"Legend","text":"<ul> <li>Dark blue: represents Personas (User, Admin)</li> <li>Blue: represents Internal Components (Frontend Web UI, LTB K8s Backend)</li> <li>Light blue: represents Components which will be implemented in this project (LTB Operator, LTB Operator API)</li> <li>Dark gray: represents External Components (K8s, Keycloak)</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/","title":"Preliminary Work (KVM/Docker)-based LTB Architecture","text":"<p>The Kubernetes based LTB was inspired by a previous implementation of the LTB, which was based on direct KVM and Docker usage.</p> <p>The following diagram shows the components and how lab nodes are deployed on a hypervisor in the KVM/Docker based LTB:</p> <p></p> <p>Currently the KVM/Docker based LTB is composed of the following containers:</p> <ul> <li>Frontend built with React</li> <li>Backend built with Django</li> <li>Databases (PostgreSQL, Redis)</li> <li>Beat</li> <li>Celery</li> <li>Web-SSH</li> <li>Prometheus</li> <li>Traefik</li> <li>Nginx</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#backend","title":"Backend","text":"<p>The backend is accessible via API and an Admin Web UI. It is responsible for the following tasks:</p> <ul> <li>parsing the yaml topology files</li> <li>deploying/destroying the containers and vms</li> <li>exposes status of lab deployments</li> <li>exposes information on how to access the deployed containers and vms</li> <li>provides remote ssh capabilities</li> <li>provides remote Wireshark capture capabilities</li> <li>managing reservations (create, delete, etc.)</li> <li>exposes node resource usage</li> <li>user management</li> <li>exposes information about a device (version, groups, etc.)</li> </ul> <p>It is composed of the following components:</p> <ul> <li>Reservations</li> <li>Running lab store</li> <li>Template store</li> <li>Authentication</li> </ul> <p>The orchestration component is responsible for creating different tasks using Celery and executing them on a remote host. There are 4 different types of tasks:</p> <ul> <li>DeploymentTask</li> <li>Deploys containers in docker</li> <li>Deploys VMs using KVM</li> <li>Creates connections between containers and VMs using an OVS bridge</li> <li>RemovalTask</li> <li>Removes a running lab</li> <li>MirrorInterfaceTask</li> <li>Creates a mirror interface on a connection</li> <li>SnapshotTask</li> <li>Takes a snapshot of a running lab</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#reservations","title":"Reservations","text":"<p>The reservation component is responsible for reserving system resources in advance. It is responsible for the following tasks:</p> <ul> <li>Create a reservation</li> <li>Delete a reservation</li> <li>Update a reservation</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#running-lab-store","title":"Running lab store","text":"<p>This component is responsible for storing information about running labs, such as:</p> <ul> <li>The devices taking part in the running lab, including the interfaces</li> <li>Connection information</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#template-store","title":"Template store","text":"<p>This component is responsible for storing lab templates.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#authentication","title":"Authentication","text":"<p>This component is responsible for user authentication and management.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#databases","title":"Databases","text":"<p>The following databases are used:</p> <ul> <li>PostgreSQL</li> <li>Redis for caching</li> </ul> <p>The Databases are used by the following components:</p> <ul> <li>Backend</li> <li>Beat</li> <li>Celery</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#beat","title":"Beat","text":"<p>The beat component is responsible for scheduling periodic tasks. To be more precise, it is responsible for scheduling the deployment and deletion of labs, according to the reservation information.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#celery","title":"Celery","text":"<p>Celery is used to execute the commands to create and delete the lab nodes and connections.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#web-ssh","title":"Web-SSH","text":"<p>Web-SSH is used to provide a web based ssh client, which can be used to access the deployed containers and vms aka lab nodes.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#prometheus","title":"Prometheus","text":"<p>Prometheus is used to collect metrics about CPU, memory and disk usage of the hypervisor nodes.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#traefik","title":"Traefik","text":"<p>Traefik is used as a proxy for the following components:</p> <ul> <li>Frontend</li> <li>Web-SSH</li> <li>Prometheus</li> <li>Nginx</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#nginx","title":"Nginx","text":"<p>Nginx is used as a reverse proxy for the backend.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#frontend","title":"Frontend","text":"<p>The frontend provides a web UI with the following features:</p> <ul> <li>User authentication</li> <li>Management of lab templates</li> <li>Management of reservations for labs</li> <li>Start/stop running labs</li> <li>Resource usage overview</li> <li>Provides information on how to access the deployed containers and vms</li> <li>Create wireshark capture interfaces</li> </ul>"},{"location":"contributions/coding-conventions/","title":"Coding Conventions","text":""},{"location":"contributions/coding-conventions/#naming","title":"Naming","text":"<p>The following naming conventions are used in the project:</p>"},{"location":"contributions/coding-conventions/#naming-conventions-in-go","title":"Naming conventions in Go","text":"<ul> <li>camelCase for variables and functions, which are not exported</li> <li>PascalCase for types and functions that need to be exported</li> </ul>"},{"location":"contributions/coding-conventions/#examples","title":"Examples","text":"<ul> <li>labInstanceStatus: variable name for a status of a lab instance</li> <li>UpdateLabInstanceStatus: name for an exported function, starts with a capital letter</li> </ul>"},{"location":"contributions/coding-conventions/#coding","title":"Coding","text":"<ul> <li>The Go extension in VSCode has a linting capability, so that will be used for linting and formatting.</li> </ul>"},{"location":"contributions/contributor-guide/","title":"Contributor Guide","text":"<p>Contributions are welcome and appreciated.</p>"},{"location":"contributions/contributor-guide/#how-it-works","title":"How it works","text":"<p>This project aims to follow the Kubernetes Operator pattern</p> <p>It uses Controllers, which provides a reconcile function responsible for synchronizing resources until the desired state is reached on the cluster.</p>"},{"location":"contributions/contributor-guide/#development-environment","title":"Development Environment","text":"<p>You\u2019ll need a Kubernetes cluster to run against. You can find instructions on how to setup your dev cluster in the Dev Cluster Setup section. Note: Your controller will automatically use the current context in your kubeconfig file (i.e. whatever cluster <code>kubectl cluster-info</code> shows).</p> <p>We recommend using VSCode with the Remote - Containers extension. This will allow you to use our devcontainer, which has all the tools needed to develop and test the operator already installed.</p>"},{"location":"contributions/contributor-guide/#prerequisites-for-recommended-ide-setup","title":"Prerequisites for recommended IDE setup","text":"<ul> <li>Docker</li> <li>VSCode</li> <li>Remote - Containers extension</li> </ul>"},{"location":"contributions/contributor-guide/#getting-started","title":"Getting Started","text":"<ol> <li>Clone the repository</li> <li>Open the repository in VSCode</li> <li>Click the popup or use the command palette to reopen the repository in a container (Dev Containers: Reopen in Container)</li> </ol> <p>Now you are ready to start developing!</p>"},{"location":"contributions/contributor-guide/#running-the-operator-locally","title":"Running the operator locally","text":"<p>You can run the operator locally on your machine, this is useful for quick testing and debugging. However, you will need to be aware that the operator will use the kubeconfig file on your machine, so you will need to make sure that the context is set to the cluster you want to run against. Therefore it also does not use the RBAC rules it would usually be deployed with.</p> <ol> <li>Install the CRDs into the cluster:</li> </ol> <pre><code>make install\n</code></pre> <ol> <li>Run your controller (this will run in the foreground, so switch to a new terminal if you want to leave it running):</li> </ol> <pre><code>make run\n</code></pre> <p>NOTE: You can also run this in one step by running: <code>make install run</code></p>"},{"location":"contributions/contributor-guide/#uninstall-crds","title":"Uninstall CRDs","text":"<p>To delete the CRDs from the cluster:</p> <pre><code>make uninstall\n</code></pre>"},{"location":"contributions/contributor-guide/#running-the-operator-on-the-cluster","title":"Running the operator on the cluster","text":"<p>You can also run the operator on the cluster, this is useful for testing the operator in a more realistic environment. However, you will first need to login to some container registry that the cluster can access, so that you can push the operator image to that registry. This will allow you to test the operators RBAC rules.</p> <p>Make sure to replace <code>&lt;some-registry&gt;</code> with the location of your container registry and <code>&lt;tag&gt;</code> with the tag you want to use.</p> <ol> <li>Install Instances of Custom Resources:</li> </ol> <pre><code>kubectl apply -f config/samples/\n</code></pre> <ol> <li>Build and push your image to the location specified by <code>IMG</code>:</li> </ol> <pre><code>make docker-build docker-push IMG=&lt;some-registry&gt;/ltb-operator:&lt;tag&gt;\n</code></pre> <ol> <li>Deploy the controller to the cluster with the image specified by <code>IMG</code>:</li> </ol> <pre><code>make deploy IMG=&lt;some-registry&gt;/ltb-operator:&lt;tag&gt;\n</code></pre>"},{"location":"contributions/contributor-guide/#undeploy-controller","title":"Undeploy controller","text":"<p>Undeploy the controller from the cluster:</p> <pre><code>make undeploy\n</code></pre>"},{"location":"contributions/contributor-guide/#modifying-the-api-definitions","title":"Modifying the API definitions","text":"<p>If you are editing the API definitions, generate the manifests such as CRs or CRDs using:</p> <pre><code>make manifests\n</code></pre> <p>NOTE: Run <code>make --help</code> for more information on all potential <code>make</code> targets</p> <p>You can find more information on how to develop the operator in the Operator SDK Documentation and the Kubebuilder Documentation</p>"},{"location":"contributions/dev-cluster-setup/","title":"Development cluster setup","text":"<p>Steps to setup a Kubernetes development cluster for testing and development of the LTB operator.</p>"},{"location":"contributions/dev-cluster-setup/#prerequisites-remote-cluster","title":"Prerequisites Remote Cluster","text":"<ul> <li>Server with Linux OS (Recommended Ubuntu 22.04)</li> </ul>"},{"location":"contributions/dev-cluster-setup/#prepare-node","title":"Prepare Node","text":"<pre><code>sudo apt update\nsudo apt upgrade -y\nsudo swapoff -a\nsudo sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#rke2-server-configuration","title":"RKE2 Server Configuration","text":"<pre><code>sudo mkdir -p /etc/rancher/rke2\nsudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <pre><code># /etc/rancher/rke2/config.yaml\nwrite-kubeconfig-mode: \"0644\"\nkube-apiserver-arg: \"allow-privileged=true\"\ncni: multus,cilium\ndisable-kube-proxy: true\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#cilium-configuration-for-multus","title":"Cilium Configuration for Multus","text":"<pre><code>sudo mkdir -p /var/lib/rancher/rke2/server/manifests\nsudo vim /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml\n</code></pre> <pre><code># /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml\n# k8sServiceHost/Port IP of Control Plane node default Port 6443\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\nname: rke2-cilium\nnamespace: kube-system\nspec:\nvaluesContent: |-\ncni:\nchainingMode: \"none\"\nexclusive: false\nkubeProxyReplacement: strict\nk8sServiceHost: \"&lt;NodeIP&gt;\"\nk8sServicePort: 6443\noperator:\nreplicas: 1\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-and-start-server-and-check-logs","title":"Install and start Server and check logs","text":"<pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=v1.26.0+rke2r2 sudo -E sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\nsudo journalctl -u rke2-server -f\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#add-kubernetes-tools-to-path-and-set-kubeconfig","title":"Add Kubernetes tools to path and set kubeconfig","text":"<p>Adds kubectl, crictl and ctr to path</p> <pre><code>echo 'export PATH=\"$PATH:/var/lib/rancher/rke2/bin\"' &gt;&gt; ~/.bashrc\necho 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bashrc\necho 'alias k=kubectl' &gt;&gt; ~/.bashrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt;~/.bashrc\nsource ~/.bashrc\nmkdir ~/.kube\nln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#get-token-for-agent","title":"Get Token for Agent","text":"<pre><code>sudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#rke2-agent-configuration-optional","title":"RKE2 Agent Configuration (Optional)","text":"<pre><code>sudo mkdir -p /etc/rancher/rke2\nsudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <pre><code># /etc/rancher/rke2/config.yaml\n---\nserver: https://&lt;server&gt;:9345\ntoken: &lt;token from server node&gt;\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-and-start-agent-and-check-logs","title":"Install and start Agent and check logs","text":"<pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" INSTALL_RKE2_VERSION=v1.26.0+rke2r2 sudo -E sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\nsudo journalctl -u rke2-agent -f\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-cluster-network-addons-operator","title":"Install Cluster Network Addons Operator","text":"<p>The Cluster Network Addons Operator can be used to deploy additional networking components. Multus and Cilium are already installed via RKE2. Open vSwitch CNI Plugin can be installed via this operator.</p> <p>First install the operator itself:</p> <pre><code>kubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/namespace.yaml\nkubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/network-addons-config.crd.yaml\nkubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/operator.yaml\n</code></pre> <p>Then you need to create a configuration for the operator example CR:</p> <pre><code>kubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/network-addons-config-example.cr.yaml\n</code></pre> <p>Wait until the operator has finished the installation:</p> <pre><code>kubectl wait networkaddonsconfig cluster --for condition=Available\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#kubevirt","title":"Kubevirt","text":"<p>Kubevirt is a Kubernetes add-on to run virtual machines.</p>"},{"location":"contributions/dev-cluster-setup/#validate-hardware-virtualization-support","title":"Validate Hardware Virtualization Support","text":"<pre><code>sudo apt install libvirt-clients\nsudo virt-host-validate qemu\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-kubevirt","title":"Install Kubevirt","text":"<p>Latest Release: <code>export RELEASE=$(curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt)</code></p> <pre><code>export RELEASE=v0.58.1\n# Deploy the KubeVirt operator\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml\n# Create the KubeVirt CR (instance deployment request) which triggers the actual installation\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml\n# wait until all KubeVirt components are up\nkubectl -n kubevirt wait kv kubevirt --for condition=Available\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-containerized-data-importer","title":"Install Containerized Data Importer","text":"<pre><code>export CDI_VERSION=v1.55.2\nkubectl create ns cdi\nkubectl -n cdi apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml\nkubectl -n cdi apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-virtctl-via-krew","title":"Install virtctl via Krew","text":"<p>First install Krew and then install virtctl via Krew</p> <pre><code>(\nset -x; cd \"$(mktemp -d)\" &amp;&amp;\nOS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\nARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\nKREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\ncurl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\ntar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n./\"${KREW}\" install krew\n)\necho 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nkubectl krew install virt\nkubectl virt help\n</code></pre> <p>You are ready to go!</p> <p>You're now ready to use the cluster for development or testing purposes.</p>"},{"location":"contributions/dev-cluster-setup/#metallb","title":"MetalLB","text":"<p>You can optionally install MetalLB, currently it is not required to use the LTB operator. MetalLB is a load-balancer implementation for bare metal Kubernetes clusters.</p>"},{"location":"contributions/dev-cluster-setup/#install-operator-lifecycle-manager-olm","title":"Install Operator Lifecycle Manager (OLM)","text":"<p>Install Operator Lifecycle Manager (OLM), a tool to help manage the Operators running on your cluster.</p> <pre><code>curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.24.0/install.sh | bash -s v0.24.0\n</code></pre> <p>Install the operator by running the following command:</p> <pre><code>kubectl create -f https://operatorhub.io/install/metallb-operator.yaml\n</code></pre> <p>This Operator will be installed in the \"operators\" namespace and will be usable from all namespaces in the cluster.</p> <p>After install, watch your operator come up using next command.</p> <pre><code>kubectl get csv -n operators\n</code></pre> <p>Now create a MetalLB IPAddressPool CR to configure the IP address range that MetalLB will use:</p> <pre><code>sudo vim metallb-ipaddresspool.yaml\n</code></pre> <pre><code># metallb-ipaddresspool.yaml\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\nname: default\nnamespace: operators\nspec:\naddresses:\n- X.X.X.X/XX\n</code></pre> <p>Create a L2Advertisement to tell MetalLB to responde to ARP requests for all IP address pools (no named ip address pool, means all pools):</p> <pre><code>sudo vim l2advertisment.yaml\n</code></pre> <pre><code># l2advertisment.yaml\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\nname: default\nnamespace: operators\nspec:\nipAddressPools:\n- default\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f metallb-ipaddresspool.yaml\nkubectl apply -f l2advertisment.yaml\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#storage","title":"Storage","text":"<p>To store your virtual machine images and disks you may want to use a storage backend. Currently no storage backend has been tested with the LTB operator, but you can try to use Trident. Trident is a dynamic storage provisioner for Kubernetes, it supports many storage backends, including NetApp, AWS, Azure, Google Cloud, and many more.</p> <p>Following you will find some instructions that may help you to install Trident on your cluster.</p> <p>You always can find more information about Trident in the official documentation.</p> <p>Check connectivity to NetApp Storage:</p> <pre><code>kubectl run -i --tty ping --image=busybox --restart=Never --rm -- \\\nping &lt;NetApp Management IP&gt;\n</code></pre> <p>Download and extract the Trident installer:</p> <pre><code>export TRIDENT_VERSION=23.01.0\nwget https://github.com/NetApp/trident/releases/download/v$TRIDENT_VERSION/trident-installer-$TRIDENT_VERSION.tar.gz\ntar -xf trident-installer-$TRIDENT_VERSION.tar.gz\ncd trident-installer\nmkdir setup\nvim ./setup/backend.json\n</code></pre> <p>Configure the installer:</p> <pre><code># ./backend.json\n{\n\"version\": 1,\n    \"storageDriverName\": \"ontap-nas\",\n    \"managementLIF\": \"&lt;NetApp Management IP&gt;\",\n    \"dataLIF\": \"&lt;NetApp Data IP&gt;\",\n    \"svm\": \"svm_k8s\",\n    \"username\": \"admin\",\n    \"password\": \"&lt;NetApp Password&gt;\",\n    \"storagePrefix\": \"trident_\",\n    \"nfsMountOptions\": \"-o nfsvers=4.1 -o mountport=2049 -o nolock\",\n    \"debug\": true\n}\n</code></pre> <p>Install Trident:</p> <pre><code>./tridentctl install -n trident -f ./setup/backend.json\n</code></pre> <p>Check the installation:</p> <pre><code>kubectl get pods -n trident\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#local-development-cluster","title":"Local development cluster","text":"<p>K3d, Minikube or Kind can be used to run a local Kubernetes cluster, if you don't have access to a remote cluster/server.</p> <p>Make sure to install the following tools:</p> <ul> <li>KubeVirt</li> <li>Containerized Data Importer</li> <li>Cluster Network Addon Operator</li> </ul> <p>KubeVirt may not work properly on local development clusters</p> <p>KubeVirt may not work properly on local development clusters, because it requires nested virtualization support, which is not available on all local development clusters. Make sure to enable nested virtualization on your local machine, if you want to run KubeVirt on a local development cluster.</p>"},{"location":"contributions/technologies-used/","title":"Tools and Frameworks","text":"<p>The tools and frameworks used in the project are listed below.</p>"},{"location":"contributions/technologies-used/#go-based-operator-sdk-framework","title":"Go-based Operator SDK framework","text":"<p>To create the LTB operator, we used the Go-based Operator-sdk framework, which provides a set of tools to simplify the process of building, testing and packaging our operator.</p>"},{"location":"contributions/technologies-used/#kubevirt","title":"Kubevirt","text":"<p>Kubevirt is a tool that provides a virtual machine management layer on top of Kubernetes. It allows us to deploy virtual machines on Kubernetes.</p>"},{"location":"contributions/technologies-used/#kubernetes","title":"Kubernetes","text":"<p>We use Kubernetes as the container orchestration platform for the LTB application.</p>"},{"location":"contributions/technologies-used/#multus-cni","title":"Multus CNI","text":"<p>To create multiple network interfaces for the pods, Multus CNI is used.</p>"},{"location":"contributions/test-concepts/","title":"Test Concept","text":""},{"location":"contributions/test-concepts/#overview","title":"Overview","text":"<p>This document outlines the approaches, methodologies, and types of tests that will be used to ensure the LTB K8s Backend components are functioning as expected.</p>"},{"location":"contributions/test-concepts/#test-categories","title":"Test categories","text":"<p>The tests will primarily focus on the following category:</p> <ul> <li>Functionality and Logic: This includes automated integration tests to evaluate how the LTB K8s Backend interacts with other components of the LTB application, such as the operator's function in a Kubernetes cluster with a K8s API server and other resources.</li> </ul> <p>Testing in the other categories, such as Security and Performance, will be considered later time and their specifics will be determined accordingly.</p>"},{"location":"contributions/test-concepts/#tools","title":"Tools","text":"<p>The tools listed below are going to be used to perform the tests mentioned above. Moreover, the tools are used in a suite test, which is created when a controller is scaffolded by the tool.</p> <ul> <li>Testify: a go package that provides a set of features to perform unit tests, such as assertions, mocks, etc.</li> <li>EnvTest: a Go library that helps write integration tests for Kubernetes controllers by setting up an instance of etcd and a Kubernetes API server, without kubelet, controller-manager, or other components.</li> <li>Ginkgo: a Go testing framework for Go to help you write epxressive, readable, and maintainable tests. It is best used with the Gomega matcher library.</li> <li>Gomega: a Go matcher library that provides a set of matchers to perform assertions in tests. It is best used with the Ginkgo testing framework.</li> </ul>"},{"location":"contributions/test-concepts/#strategies-test-approach","title":"Strategies: Test Approach","text":"<p>The following test approaches are going to be used to test the LTB K8s Backend components:</p>"},{"location":"contributions/test-concepts/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are going to be used to test small pieces of code, such as functions, which don't involve setting up testing Kubernetes environment with a K8s API server and other resources.</p>"},{"location":"contributions/test-concepts/#integration-tests","title":"Integration Tests","text":"<p>Integration tests are going to be used to test the different components of the LTB K8s Backend, such as the operator, the controllers, etc., and how they interact with each other.</p>"},{"location":"contributions/test-concepts/#environment","title":"Environment","text":"<p>EnvTest is going to be used to set up a testing Kubernetes environment with a K8s API server and other resources.</p>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/","title":"Use Markdown Architectural Decision Records","text":""},{"location":"decisions/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record design decisions made in this project. Which format and structure should these records follow?</p>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR 2.1.2 \u2013 The Markdown Architectural Decision Records</li> <li>Michael Nygard's template \u2013 The first incarnation of the term \"ADR\"</li> <li>Sustainable Architectural Decisions \u2013 The Y-Statements</li> <li>Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 2.1.2\", because</p> <ul> <li>Implicit assumptions should be made explicit.   Design documentation is important to enable people understanding the decisions later on.   See also A rational design process: How and why to fake it.</li> <li>The MADR format is lean and fits our development style.</li> <li>The MADR structure is comprehensible and facilitates usage &amp; maintenance.</li> <li>The MADR project is vivid.</li> <li>Version 2.1.2 is the latest one available when starting to document ADRs.</li> <li>A Visual Studio Code extension ADR Manager for MADR exits, which makes managing ADRs easy.</li> </ul>"},{"location":"decisions/0001-operator-SDK/","title":"Operator SDK","text":""},{"location":"decisions/0001-operator-SDK/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>It's best practice to use an SDK to build operators for Kubernetes. The SDK provides a higher level of abstraction for creating Kubernetes operators, which makes it easier to write and manage operators. There are multiple SDKs available for building operators. We need a SDK that's flexible and easy to use and can be used with Go.</p>"},{"location":"decisions/0001-operator-SDK/#considered-options","title":"Considered Options","text":"<ul> <li>Operator SDK (Operator Framework)</li> <li>KubeBuilder</li> <li>Kopf</li> <li>KUDO</li> <li>Metacontroller</li> </ul>"},{"location":"decisions/0001-operator-SDK/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Operator SDK\", because it provides a high level of abstraction for creating Kubernetes operators, which makes it easier to write and manage operators. Additionally, there are tools and libraries for building and testing the operator included in Operator SDK, it's easy to use and can be used with Go.</p>"},{"location":"decisions/0001-operator-SDK/#links","title":"Links","text":"<ul> <li>Operator SDK</li> <li>Tools to build an operator</li> </ul>"},{"location":"decisions/00011-remote-access/","title":"Remote-Access","text":""},{"location":"decisions/00011-remote-access/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>For the lab instances to be useful for the students, they need to be able to access the pods (containers) and VMs. Access to pods/VMs should only be granted to user with the appropriate access rights. It should be possible to access the pods/VMs console and or access it via multiple OOB protocols (SSH, RDP, VNC, etc.).</p>"},{"location":"decisions/00011-remote-access/#considered-options","title":"Considered Options","text":"<ul> <li>Kubernetes Service</li> <li>Gotty</li> <li>ttyd</li> </ul>"},{"location":"decisions/00011-remote-access/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"ttyd and Kubernetes Service\", ttyd will be used as a jump host to access the pods/VMs console. A Kubernetes service will be used to allow access the pods/VMs via OOB protocols. Security for the console access will likely be easy to implement. Secure access via OOB protocols was considered, but will need to be researched further, currently it would depend on the OOB protocol used and the security features it provides.</p>"},{"location":"decisions/0002-operator-scope/","title":"Operator Scope","text":""},{"location":"decisions/0002-operator-scope/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The Operator could be a namespace-scoped or cluster-scoped.</p>"},{"location":"decisions/0002-operator-scope/#considered-options","title":"Considered Options","text":"<ul> <li>Namespace-scoped</li> <li>Cluster-scoped</li> </ul>"},{"location":"decisions/0002-operator-scope/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Cluster-scoped\", because cluster-scoped operators enable you to manage namespaces or resources in the entire cluster. This is needed to ensure that each lab instance can be deployed within its own namespace. Cluster-scoped operators are also capable of managing infrastructure-level resources, such as nodes. Additionally, cluster-scoped operators provide greater visibility and control over the entire cluster.</p>"},{"location":"decisions/0002-operator-scope/#links","title":"Links","text":"<ul> <li>Operator Scope</li> </ul>"},{"location":"decisions/0003-api-and-operator-deployment/","title":"API and Operator Deployment","text":""},{"location":"decisions/0003-api-and-operator-deployment/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The operator and the API could be separated and deployed as two services/containers or they could be deployed as one service/container.</p>"},{"location":"decisions/0003-api-and-operator-deployment/#considered-options","title":"Considered Options","text":"<ul> <li>One container</li> <li>Separate containers</li> </ul>"},{"location":"decisions/0003-api-and-operator-deployment/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Separate containers\", because it provides more flexibility and scalability. It also makes it easier to update the operator and the API separately. Additionally, it is easy to separate the API and the operator into two different services, as they talk to each other via the Kubernetes API.</p>"},{"location":"decisions/0004-dev-container/","title":"Dev Container","text":""},{"location":"decisions/0004-dev-container/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Every team member could set up their development environment manually or we could use a dev container to provide a consistent development environment for all team members and future contributors.</p>"},{"location":"decisions/0004-dev-container/#considered-options","title":"Considered Options","text":"<ul> <li>Dev Container</li> <li>Manual Setup</li> </ul>"},{"location":"decisions/0004-dev-container/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Dev Container\", because a dev container setup lets you create the same development environment for all team members, which ensures consistency. It also provides a completely isolated development environment, which helps to avoid software incompatibility issues, such as operator-sdk not working on Windows. Moreover, a dev container is easily portable and works on all operating systems that support Docker. The only downside is that not all IDEs support dev containers, but at least two of the currently most popular IDEs VS Code and Visual Studio support dev containers.</p>"},{"location":"decisions/0004-dev-container/#links","title":"Links","text":"<ul> <li>DevContainer</li> <li>Most Popular IDEs</li> </ul>"},{"location":"decisions/0005-replace-current-ltb-backend/","title":"Replace Current LTB Backend","text":""},{"location":"decisions/0005-replace-current-ltb-backend/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The LTB K8s Backend could replace the current LTB Backend fully or partially by leaving features, such as User-Management in the current LTB Backend.</p>"},{"location":"decisions/0005-replace-current-ltb-backend/#considered-options","title":"Considered Options","text":"<ul> <li>Replace current LTB Backend fully</li> <li>Replace current LTB Backend partially</li> </ul>"},{"location":"decisions/0005-replace-current-ltb-backend/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Replace current LTB Backend fully\", because huge parts of the current LTB Backend would need to be rewritten to be compatible with the new LTB K8s operator and it would be easier to rewrite the whole backend. Additionally, the same programming language can be used throughout the whole Backend, Go.</p>"},{"location":"decisions/0006-lab-instance-set/","title":"Lab Instance Set","text":""},{"location":"decisions/0006-lab-instance-set/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We could create a CR called LabInstanceSet, and tell the operator we want e.g. 10 LabInstances by providing one LabInstance CR and a generator like a list of names or we could provide the operator 10 CRs to create 10 LabInstances.</p>"},{"location":"decisions/0006-lab-instance-set/#considered-options","title":"Considered Options","text":"<ul> <li>With LabInstanceSet</li> <li>Without LabInstanceSet</li> </ul>"},{"location":"decisions/0006-lab-instance-set/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Without LabInstanceSet\", because we currently don't see a need for it. This could change in the future, but for now we will not implement it.</p>"},{"location":"decisions/0007-interaction-with-operator/","title":"Interaction with Operator","text":""},{"location":"decisions/0007-interaction-with-operator/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>There are multiple ways to interact with the operator, such as a GitOps approach, using the frontend via the API or using <code>kubectl</code>.</p>"},{"location":"decisions/0007-interaction-with-operator/#considered-options","title":"Considered Options","text":"<ul> <li>GitOps</li> <li>Use frontend</li> <li>Use kubectl</li> <li>All</li> </ul>"},{"location":"decisions/0007-interaction-with-operator/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"All\", because these options are not mutually exclusive and can be used together, we want to support all of them.</p>"},{"location":"decisions/0008-namespace-per-labinstance/","title":"Namespace Per LabInstance","text":""},{"location":"decisions/0008-namespace-per-labinstance/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>LabInstances could be created in separate namespaces or one namespace for all LabInstances.</p>"},{"location":"decisions/0008-namespace-per-labinstance/#considered-options","title":"Considered Options","text":"<ul> <li>One Namespace for all LabInstances</li> <li>Namespace per LabInstance</li> </ul>"},{"location":"decisions/0008-namespace-per-labinstance/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Namespace per LabInstance\", because it will be easier in the future to implement features like RBAC and resource quotas and limits.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/","title":"K8s Aggreted API Over Standalone API","text":""},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want a declarative way of creating LTB labs inside Kubernetes using Kubernetes native pods and KubeVirt virtual machines. We could either create a standalone API which interacts with the Kubernetes API and does not follow the Kubernetes API conventions. And therefore is not compatible with Kubernetes tools, such as dashboards or <code>kubectl</code>, but would allow complete control over the API design. Or we could create an aggregated API which uses the Kubernetes aggregation layer to extend the Kubernetes API, which would allow us to use Kubernetes tools, such as dashboards or <code>kubectl</code>, but would limit the control over the API design.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#considered-options","title":"Considered Options","text":"<ul> <li>Standalone API</li> <li>Aggregated API</li> </ul>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Aggregated API\", because Is better suited for declerative API, our new types will be readable and writeable using kubectl/Kubernetes tools, such as dashboards. We also can leverage Kuberbetes API support features this way. Additionally, our resources are scoped to a cluster or namespaces of a cluster. Finally, the Operator Pattern is simpler to implement this way.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#links","title":"Links","text":"<ul> <li>Standalone vs Aggregated API</li> <li>Aggregated API</li> <li>Operator Pattern</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/","title":"Extending LTB with New Node Types","text":""},{"location":"decisions/0010-extending-ltb-with-new-node-types/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>It should be possible to create, update and delete node types (e.g. Ubuntu, XRD, XR, IOS, Cumulus, etc.) Node types should be used inside lab templates and expose a way to provide a node with configuration (cloud-init, zero-touch, etc.) The amount of available network interfaces is dynamic and depends on how many connections a node has according to a specific lab template.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Certain operating systems' images like XR, and XRD need a specific interface configuration which depends on how many interfaces a certain node will receive.</li> <li>The chosen solution should support multiple version of a type in an easy to use way (e.g. Ubuntu 22.04, 20.04, ...).</li> <li>For XRd images, interfaces need to have environment variables set for each interface they use, and the interface count needs to be dynamically set according to the lab template.</li> <li>For XR VM images, the first interface is the management interface and then there are two empty interfaces that need a special configuration.</li> <li>For mount from config might be different</li> <li>Cumulus VX images need a privileged container</li> <li>XRd need additional privileges</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#considered-options","title":"Considered Options","text":"<ul> <li>Custom Resources</li> <li>Go</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Custom Resources\", because it will be possible to support all the cases mentioned in the decision drivers using go templates and CRs. Implementing the types in Go does not seem to bring any major advantages, whereas using CRs will be easier for external users to extend the system with new node types.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Easy to extend during runtime</li> <li>Easy to extend for external users</li> <li>All decision drivers will be supported</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Go Templates are not as powerful as Go, which could make it harder to implement certain node types.</li> <li>A Custom Resource is also a little bit less flexible than a Go type, but this should not be a problem for the use cases we have.</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#links","title":"Links","text":"<ul> <li>Example of similar solution using Go node types</li> </ul>"},{"location":"decisions/0011-programming-language/","title":"Programming Language","text":""},{"location":"decisions/0011-programming-language/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need to choose a programming language for the project. The considered options are based on the supported languages of the Operator SDK.</p>"},{"location":"decisions/0011-programming-language/#considered-options","title":"Considered Options","text":"<ul> <li>Go</li> <li>Helm</li> <li>Ansible</li> </ul>"},{"location":"decisions/0011-programming-language/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Go\", because many cloud native projects are written in Go, and it is a compiled language, which is more performant than interpreted languages. Also, Go is a statically typed language, which makes it easier to maintain and refactor the code. It is also easier to write complicated logic and tests in Go than in Helm or Ansible.</p>"}]}