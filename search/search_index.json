{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lab Topology Builder Documentation","text":"<p>The Lab Topology Builder (LTB) is an open source project that allows users to deploy networking labs on Kubernetes. It is a tool that enables you to build a topology of virtual machines and containers, which are connected to each other according to the network topology you have defined.</p> <p>To get started, please refer to the User Guide.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Management of networking labs on Kubernetes</li> <li>Status querying of lab deployments</li> <li>Remote access to lab nodes' console via web browser</li> <li>Remote access to lab nodes' OOB management (e.g. SSH)</li> <li>Management of custom node types</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"about/","title":"About","text":"<p>This project was created as part of a bachelor thesis at the Eastern Switzerland University of Applied Sciences in Rapperswil, in cooperation with the Institue for Network and Security.</p>"},{"location":"about/#authors","title":"Authors","text":"<ul> <li>Jan Untersander</li> <li>Tsigereda Nebai Kidane</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#packages","title":"Packages","text":"<ul> <li>ltb-backend.ltb/v1alpha1</li> </ul>"},{"location":"api-reference/#ltb-backendltbv1alpha1","title":"ltb-backend.ltb/v1alpha1","text":""},{"location":"api-reference/#resource-types","title":"Resource Types","text":"<ul> <li>LabInstance</li> <li>LabTemplate</li> <li>NodeType</li> </ul>"},{"location":"api-reference/#labinstance","title":"LabInstance","text":"<p>A lab instance is created as a specific instance of a deployed lab, using the configuration from the corresponding lab template.</p> Field Description <code>apiVersion</code> string <code>ltb-backend.ltb/v1alpha1</code> <code>kind</code> string <code>LabInstance</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> LabInstanceSpec"},{"location":"api-reference/#labinstancenodes","title":"LabInstanceNodes","text":"<p>Configuration for a lab node.</p> <p>Appears in: - LabTemplateSpec</p> Field Description <code>name</code> string The name of the lab node. <code>nodeTypeRef</code> NodeTypeRef The type of the lab node. <code>interfaces</code> NodeInterface array Array of interface configurations for the lab node. (currently not supported) <code>config</code> string The configuration for the lab node. <code>ports</code> Port array Array of ports which should be publicly exposed for the lab node."},{"location":"api-reference/#labinstancespec","title":"LabInstanceSpec","text":"<p>LabInstanceSpec define which LabTemplate should be used for the lab instance and the DNS address.</p> <p>Appears in: - LabInstance</p> Field Description <code>labTemplateReference</code> string Reference to the name of a LabTemplate to use for the lab instance. <code>dnsAddress</code> string The DNS address, which will be used to expose the lab instance. It should point to the Kubernetes node where the lab instance is running."},{"location":"api-reference/#labtemplate","title":"LabTemplate","text":"<p>Defines the lab topology, its nodes and their configuration.</p> Field Description <code>apiVersion</code> string <code>ltb-backend.ltb/v1alpha1</code> <code>kind</code> string <code>LabTemplate</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> LabTemplateSpec"},{"location":"api-reference/#labtemplatespec","title":"LabTemplateSpec","text":"<p>LabTemplateSpec defines the Lab nodes and their connections.</p> <p>Appears in: - LabTemplate</p> Field Description <code>nodes</code> LabInstanceNodes array Array of lab nodes and their configuration. <code>neighbors</code> string array Array of connections between lab nodes. (currently not supported)"},{"location":"api-reference/#nodeinterface","title":"NodeInterface","text":"<p>Interface configuration for the lab node (currently not supported)</p> <p>Appears in: - LabInstanceNodes</p> Field Description <code>ipv4</code> string IPv4 address of the interface. <code>ipv6</code> string IPv6 address of the interface."},{"location":"api-reference/#nodetype","title":"NodeType","text":"<p>NodeType defines a type of node that can be used in a lab template</p> Field Description <code>apiVersion</code> string <code>ltb-backend.ltb/v1alpha1</code> <code>kind</code> string <code>NodeType</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> NodeTypeSpec"},{"location":"api-reference/#nodetyperef","title":"NodeTypeRef","text":"<p>NodeTypeRef references a NodeType with the possibility to provide additional information to the NodeType.</p> <p>Appears in: - LabInstanceNodes</p> Field Description <code>type</code> string Reference to the name of a NodeType. <code>image</code> string Image to use for the NodeType. Is available as variable in the NodeType and functionality depends on its usage. <code>version</code> string Version of the NodeType. Is available as variable in the NodeType and functionality depends on its usage."},{"location":"api-reference/#nodetypespec","title":"NodeTypeSpec","text":"<p>NodeTypeSpec defines the Kind and NodeSpec for a NodeType</p> <p>Appears in: - NodeType</p> Field Description <code>kind</code> string Kind can be used to specify if the nodes is either a pod or a vm <code>nodeSpec</code> string NodeSpec is the PodSpec or VirtualMachineSpec configuration for the node with the possibility to use go templating syntax to include LabTemplate variables (see User Guide) See PodSpec and VirtualMachineSpec"},{"location":"api-reference/#port","title":"Port","text":"<p>Port of a lab node which should be publicly exposed.</p> <p>Appears in: - LabInstanceNodes</p> Field Description <code>name</code> string Arbitrary name for the port. <code>protocol</code> Protocol Choose either TCP or UDP. <code>port</code> integer The port number to expose."},{"location":"comparison/","title":"Comparison to other similar projects","text":"<p>The Lab Topology Builder is just one among many open-source projects available for building emulated network topologies. The aim of this project comparison is to provide a concise overview of the key features offered by some of the most well-known projects, assisting users in selecting the optimal solution for their use case.</p>"},{"location":"comparison/#vrnetlab-vr-network-lab","title":"vrnetlab - VR Network Lab","text":"<p>vrnetlab is a network emulator that runs virtual routers using KVM and Docker. It is similar to the KVM/Docker-based LTB, but is more simple and only provides the deployment functionality.</p>"},{"location":"comparison/#containerlab","title":"Containerlab","text":"<p>Containerlab is a tool to deploy network labs with virtual routers, firewalls, load balancers, and more, using Docker containers. It is based on vrnetlab and provides a declarative way to define the lab topology using a YAML file. Containerlab is not capable of deploying lab topologies over multiple host nodes, which is a key feature that the K8s-based LTB aims to provide in the future.</p>"},{"location":"comparison/#netlab","title":"Netlab","text":"<p>Netlab is an abstraction layer to deploy network topologies based on containerlab or Vagrant. It provides a declarative way to define the lab topology using a YAML file. It mainly provides an abstracted way to define lab topologies with preconfigured lab nodes.</p>"},{"location":"comparison/#kubernetes-network-emulator","title":"Kubernetes Network Emulator","text":"<p>Kubernetes Network Emulator is a network emulator that aims to provide a standard interface so that vendors can produce a standard container implementation of their network operating system that can be used in a network emulation environment. Currently, it does not seem to support many network operating systems and additional operators are required to support different vendors.</p>"},{"location":"comparison/#mininet","title":"Mininet","text":"<p>Minitnet is a network emulator that runs a collection of end-hosts, switches, routers, and links on a single Linux kernel. It is mainly used for testing SDN controllers and can not deploy a lab with a specific vendor image.</p>"},{"location":"comparison/#gns3","title":"GNS3","text":"<p>GNS3 is a network emulator that can run network devices as virtual machines or Docker containers. It primarily focuses on providing an emulated network environment for a single user and its deployment and usage can be quite complex. Additionally, it does not provide a way to scale labs over multiple host nodes.</p>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#lab-topology-builder","title":"Lab Topology Builder","text":"<p>The Lab Topology Builder is a network emulator that allows you to build a topology of virtual machines and containers, which are connected to each other according to the network topology you have defined.</p>"},{"location":"concepts/#network-topology","title":"Network Topology","text":"<p>The arrangement or pattern in which all nodes on a network are connected together is referred to as the network\u2019s topology.</p> <p>Here is an example of a network topology:</p> <p></p>"},{"location":"concepts/#lab","title":"Lab","text":"<p>In our context, a lab refers to a networking lab consisting of interconnected nodes following a specific network topology.</p>"},{"location":"concepts/#ltb-operator","title":"LTB Operator","text":"<p>The LTB Operator is a K8s Operator for the LTB application, which is responsible for creating, configuring, and managing the emulated network topologies of the LTB application inside a Kubernetes cluster. It also automatically updates the status of the labs based on the current state of the associated containers and virtual machines, ensuring accurate and real-time lab information.</p>"},{"location":"concepts/#lab-template","title":"Lab Template","text":"<p>Lab Template is a CR, which defines a template for a lab. It contains information about which nodes are part of the lab, their configuration and how they are connected to each other.</p>"},{"location":"concepts/#lab-instance","title":"Lab Instance","text":"<p>A lab instance is a CR that describes a lab that you want to deploy in a Kubernetes cluster. It has a reference to the lab template you want to use and also has a status field that is updated by the operator, which shows how many pods and VMs are running in the lab and the status of the lab instance itself. In addition, it also has a dns address field, which will be used to access the nodes using the web-based terminal.</p>"},{"location":"concepts/#node-type","title":"Node Type","text":"<p>In a network, a node represents any device that is part of the lab. A NodeType is a CR that defines a type of node that can be part of a lab. You reference the node type you want to have in your lab in the lab template. Within LTB, a node can be either a KubeVirt virtual machine or a regular Kubernetes pod.</p>"},{"location":"concepts/#links","title":"Links","text":"<p>If you would like to familiarize yourself with the concepts mentioned below, please refer to the following links:</p> <ul> <li>Kubernetes</li> <li>Kubernetes Cluster</li> <li>Kubernetes Operator</li> <li>Custom Resource Definition</li> <li>Custom Resource</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":""},{"location":"user-guide/#installation-pre-requisites","title":"Installation Pre-requisites","text":"Tool Version Installation Description Kubernetes ^1.26.0 Installation Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubevirt 0.59.0 Installation Kubevirt is a Kubernetes add-on to run virtual machines on Kubernetes. Multus-CNI 3.9.0 Installation Multus-CNI is a plugin for K8s to attach multiple network interfaces to pods. Operator Lifecycle Manager ^0.24.0 Installation Operator Lifecycle Manager (OLM) helps users install, update, and manage the lifecycle of all Operators and their associated services running across their Kubernetes clusters. <p>Alternative OLM installation:</p> <pre><code>curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.25.0/install.sh | bash -s v0.25.0\n</code></pre> <p>Change the version to the desired one.</p>"},{"location":"user-guide/#installation-of-the-ltb-k8s-operator","title":"Installation of the LTB K8s operator","text":"<ol> <li> <p>Install the operator by creating a catalog source and subscription. <pre><code>kubectl apply -f https://raw.githubusercontent.com/Lab-Topology-Builder/LTB-K8s-Backend/main/install/catalogsource.yaml -f https://raw.githubusercontent.com/Lab-Topology-Builder/LTB-K8s-Backend/main/install/subscription.yaml\n</code></pre></p> </li> <li> <p>Wait for the operator to be installed <pre><code>kubectl get csv -n operators -w\n</code></pre></p> </li> </ol>"},{"location":"user-guide/#usage","title":"Usage","text":"<p>To create a lab you'll need to create at least one node type and one lab template. Node types define the basic properties of a node. For VMs this includes everything that can be defined in a Kubevirt VirtualMachineSpec and for pods everything that can be defined in a Kubernetes PodSpec.</p> <p>In order to provide better reusability of node types, you can use Go templating Syntax to include information from the lab template (like configuration or node name) in the node type. The following example node types show how this can be done. You can use them as a starting point for your own node types.</p>"},{"location":"user-guide/#example-node-type","title":"Example Node Type","text":"<p>This is an example of a VM node type. It creates a VM with 2 vCPUs and 4GB of RAM, using the Ubuntu 22.04 container disk image from quay.io/containerdisks/ubuntu and the <code>cloudInitNoCloud</code> volume source to provide a cloud-init configuration to the VM.</p> <p>Everything that is defined in the <code>node</code> field of the lab template is available to the node type via the <code>.</code> variable. Example: <code>{{ .Name }}</code> will be replaced with the name of the node from the lab template.</p> <p>Currently, you cannot provide the cloud-init configuration as a YAML string via the .Config field of the lab template. Instead, you have to encode it as base64 string and therefore use the <code>userDataBase64</code> field of the volume source, because of indentation issues while rendering configuration.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: NodeType\nmetadata:\nname: nodetypeubuntuvm\nspec:\nkind: vm\nnodeSpec: |\nrunning: true\ntemplate:\nspec:\ndomain:\nresources:\nrequests:\nmemory: 4096M\ncpu:\ncores: 2\ndevices:\ndisks:\n- name: containerdisk\ndisk:\nbus: virtio\n- name: cloudinitdisk\ndisk:\nbus: virtio\nterminationGracePeriodSeconds: 0\nvolumes:\n- name: containerdisk\ncontainerDisk:\nimage: quay.io/containerdisks/ubuntu:22.04\n- name: cloudinitdisk\ncloudInitNoCloud:\nuserDataBase64: {{ .Config }}\n</code></pre> <p>This is an example of a generic pod node type. It creates a pod with a single container. The container name, container image, command and ports to expose are taken from the lab template.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: NodeType\nmetadata:\nname: genericpod\nspec:\nkind: pod\nnodeSpec: |\ncontainers:\n- name: {{ .Name }}\nimage: {{ .NodeTypeRef.Image}}:{{ .NodeTypeRef.Version }}\ncommand: {{ .Config }}\nports:\n{{- range $index, $port := .Ports }}\n- name: {{ $port.Name }}\ncontainerPort: {{ $port.Port }}\nprotocol: {{ $port.Protocol }}\n{{- end }}\n</code></pre> <p>After you have defined some node types, you can create a lab template. A lab template defines the nodes that should be created for a lab, how they should be configured and how they should be connected.</p>"},{"location":"user-guide/#example-lab-template","title":"Example Lab Template","text":"<p>This is an example of a lab template, which you can use as a starting point for your own labs. It uses the previously defined node types to create a VM and two pods. They are referenced via the <code>nodeTypeRef</code> field. The provided ports will be exposed to the host network and can be accessed via the node's IP address and the port number assigned by Kubernetes. You can retrieve the IP address of a node by running <code>kubectl get node -o wide</code> and the port number by running <code>kubectl get svc</code>.</p> <p>Currently, there is no support for point to point connections between nodes. Instead, they are all connected to the same network. In the future, we plan to add support for point to point connections, which will be able to be defined as neighbors in the lab template. The syntax for this is not yet definite, but it will probably look something like this:</p> <pre><code>  neighbors:\n- \"sample-node-1:1,sample-node-2:1\"\n- \"sample-node-2:2-sample-node-3:1\"\n</code></pre> <p>This would connect the first port of <code>sample-node-1</code> to the first port of <code>sample-node-2</code> and the second port of <code>sample-node-2</code> to the first port of <code>sample-node-3</code>.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabTemplate\nmetadata:\nname: labtemplate-sample\nspec:\nnodes:\n- name: \"sample-node-1\"\nnodeTypeRef:\ntype: \"nodetypeubuntuvm\"\nconfig: \"I2Nsb3VkLWNvbmZpZwpwYXNzd29yZDogdWJ1bnR1CmNocGFzc3dkOiB7IGV4cGlyZTogRmFsc2UgfQpzc2hfcHdhdXRoOiBUcnVlCnBhY2thZ2VzOgogLSBxZW11LWd1ZXN0LWFnZW50CiAtIGNtYXRyaXgKcnVuY21kOgogLSBbIHN5c3RlbWN0bCwgc3RhcnQsIHFlbXUtZ3Vlc3QtYWdlbnQgXQo=\"\nports:\n- name: \"ssh\"\nport: 22\nprotocol: \"TCP\"\n- name: \"sample-node-2\"\nnodeTypeRef:\ntype: \"genericpod\"\nimage: \"ghcr.io/insrapperswil/network-ninja\"\nversion: \"latest\"\nports:\n- name: \"ssh\"\nport: 22\nprotocol: \"TCP\"\nconfig: '[\"/bin/bash\", \"-c\", \"apt update &amp;&amp; apt install -y openssh-server &amp;&amp; service ssh start &amp;&amp; sleep 365d\"]'\n- name: \"sample-node-3\"\nnodeTypeRef:\ntype: \"genericpod\"\nimage: \"ubuntu\"\nversion: \"22.04\"\nports:\n- name: \"ssh\"\nport: 22\nprotocol: \"TCP\"\nconfig: '[\"/bin/bash\", \"-c\", \"apt update &amp;&amp; apt install -y openssh-server &amp;&amp; service ssh start &amp;&amp; sleep 365d\"]'\n</code></pre> <p>With the lab template defined, you can create a lab instance.</p>"},{"location":"user-guide/#example-lab-instance","title":"Example Lab Instance","text":"<p>This is an example of lab instance, which you can use as a starting point for your own labs. The lab instance references the previously defined lab template with the <code>labTemplateReference</code> field. You also need to provide a DNS address via the <code>dnsAddress</code> field. This address will be used to create routes for the web terminal to the lab nodes. For example, if you use the address <code>example.com</code>, the console of a node called <code>sample-node-1</code> will be available at <code>https://labinstance-sample-sample-node-1.example.com/</code> via a web terminal.</p> <p>Currently, there is no support to edit the lab instance after it has been created. If you want to change the lab, you have to delete the lab instance and create a new one.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabInstance\nmetadata:\nname: labinstance-sample\nspec:\nlabTemplateReference: \"labtemplate-sample\"\ndnsAddress: \"example.com\"\n</code></pre>"},{"location":"user-guide/#uninstall","title":"Uninstall","text":"<ol> <li> <p>Delete the subscription <pre><code>kubectl delete subscriptions.operators.coreos.com -n operators ltb-subscription\n</code></pre></p> </li> <li> <p>Delete the CSV <pre><code>kubectl delete csv -n operators ltb-operator.&lt;version&gt;\n</code></pre></p> </li> <li> <p>Delete the CRDs <pre><code>kubectl delete crd labinstances.ltb-backend.ltb labtemplates.ltb-backend.ltb nodetypes.ltb-backend.ltb\n</code></pre></p> </li> <li> <p>Delete operator <pre><code>kubectl delete operator ltb-operator.operators\n</code></pre></p> </li> <li> <p>Delete the CatalogSource <pre><code>kubectl delete catalogsource.operators.coreos.com -n operators ltb-catalog\n</code></pre></p> </li> </ol>"},{"location":"architecture/k8s-ltb-architecture/","title":"Kubernetes Lab Topology Builder Architecture","text":"<p>The main components of the Kubernetes based LTB are:</p> <ul> <li>Frontend</li> <li>API</li> <li>Operator</li> </ul> <p>The following diagram shows how the components interact with each other:</p> <p></p>"},{"location":"architecture/k8s-ltb-architecture/#frontend","title":"Frontend","text":"<p>The frontend can be implemented in any language and framework, it just needs to be able to communicate via an HTTP API with the LTB API. The frontend is responsible for the following tasks:</p> <ul> <li>Providing a web UI for the user to interact with the labs.</li> <li>Providing a web UI for the admin to manage:</li> <li>Lab templates</li> <li>Lab deployments</li> <li>Reservations</li> </ul> <p>There is a possibility to reuse parts of the existing frontend from the KVM/Docker-based LTB.</p>"},{"location":"architecture/k8s-ltb-architecture/#api","title":"API","text":"<p>The API is responsible for the following tasks:</p> <ul> <li>To create, update and delete LTB resources (node types, lab templates, lab instances)</li> <li>Exposes status of lab instances</li> <li>Expose information on how to access the deployed lab nodes</li> <li>Authentication via an external authentication provider</li> </ul> <p>No parts from the existing KVM/Docker-based LTB can be reused for the API.</p>"},{"location":"architecture/k8s-ltb-architecture/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>The authentication can be implemented by using an external authentication provider like Keycloak. Keycloak can be configured to act as an authentication broker with external identity providers like LDAP, OpenID Connect, SAML, etc. This has the benefit that the LTB does not need to implement any authentication logic and can focus on the lab deployment. Additionally, it enables the LTB to be integrated into an existing authentication infrastructures, with the benefit that users do not need to create a new account. On the other hand, it has the drawback that the LTB needs an external authentication provider to work and that the users access rights would need to be managed in Keycloak.</p> <p>Authorization can also be implemented using Keycloak and its Authorization Services.</p>"},{"location":"architecture/k8s-ltb-architecture/#operator","title":"Operator","text":"<p>The operator is responsible for the following tasks:</p> <ul> <li>Deploying/destroying the containers and vms</li> <li>Check validity of LTB resources (node types, lab templates, lab instances)</li> <li>Enables you to access the deployed containers and vms via different protocols</li> <li>Providing remote access to the lab node console via a web terminal</li> <li>Managing reservations (create, delete, etc.)</li> <li>Providing remote Wireshark capture capabilities</li> </ul> <p>The operator is implemented according to the Kubernetes operator pattern. It has multiple controllers that are responsible for managing a particular custom resource like lab template.</p>"},{"location":"architecture/k8s-ltb-architecture/#network-connectivity-between-lab-nodes","title":"Network connectivity between lab nodes","text":"<p>The network connectivity between lab nodes can be implemented with Multus, which is a \"meta-plugin\" that enables attaching multiple CNI plugins to a kubernetes pod/vm. Multus uses NetworkAttachmentDefinitions (NAD) to describe, which CNI plugin should be used and how it should be configured.</p> <p>Currently we use a linux bridge as a secondary CNI plugin, with the drawback that the links between the lab nodes are not pure layer 2 links, but layer 3 links. Additionally the connection between the lab nodes only work on the same Kubernetes host, because the linux bridge does not implement any kind of cross-host networking.</p>"},{"location":"architecture/k8s-ltb-architecture/#remote-access-to-lab-nodes","title":"Remote access to lab nodes","text":"<p>Remote access to the lab nodes has two variants:</p> <ul> <li>Console access via a web terminal</li> <li>Access to configurable ports with any OOB management protocol</li> </ul>"},{"location":"architecture/k8s-ltb-architecture/#console-access-via-a-web-terminal","title":"Console access via a web terminal","text":"<p>The console access via a web terminal is implemented with kube-ttyd, which is a tool based on ttyd, with the addition to use kubectl exec and virsh console to connect to the lab nodes. <code>kube-ttyd</code> was provided by Yannick Zwicker from the INS specifically for this project. Access to the web terminal is routed through an NGINX ingress controller, and a Kubernetes service of type <code>ClusterIP</code>.</p> <p>The authentication feature of the NGINX ingress controller can be used to restrict access to the web terminal to authenticated users. It might be possible to use the same authentication provider as the LTB API, but this needs to be tested.</p>"},{"location":"architecture/k8s-ltb-architecture/#access-to-configurable-ports-with-any-oob-management-protocol","title":"Access to configurable ports with any OOB management protocol","text":"<p>Access to lab nodes via freely choosable OOB management protocols is implemented by providing a Kubernetes service of type <code>LoadBalancer</code> for each lab node, which is configured to expose the ports specified in the lab template.</p> <p>Access control needs to be implemented by the lab node itself, because the Kubernetes service of type <code>LoadBalancer</code> does not provide any authentication or authorization features. An example for this would be to provide SSH keys for the lab nodes inside the lab template config field.</p>"},{"location":"architecture/k8s-ltb-architecture/#scheduling-lab-instances-and-resource-reservation","title":"Scheduling lab instances and resource reservation","text":"<p>A feature to schedule the deployment and deletion of a lab instance to a specific time is not implemented, but could be implemented by adding an additional fields (creationTimestamp, deletionTimestamp) to the lab instance custom resource. The lab instance controller then is able to check these fields and deploy or delete the lab instance at the specified time. There are multiple ways to implement this, either by regularly checking the lab instance custom resources, or by requeuing the lab instance creation/deletion event to the specified time.</p> <p>If there are any issues with the requeuing of these events over such a long period of time, writing a Kubernetes informer could be a solution.</p> <p>Resource reservation in a capacity planning sense is not provided by Kubernetes, a manual solution could be implemented by using limit ranges, resource quotas and the Kubernetes node resources. Planned resource management is a huge topic, and we would recommend to create a dedicated project for this.</p>"},{"location":"architecture/k8s-ltb-architecture/#comparison-to-the-kvmdocker-based-ltb","title":"Comparison to the KVM/Docker-based LTB","text":"<p>The diagram below illustrates the components of the KVM/Docker-based LTB, highlighting the changes introduced by the Kubernetes LTB.</p> <p></p>"},{"location":"architecture/k8s-ltb-architecture/#c4-model","title":"C4 Model","text":"<p>The following diagrams show the C4 model of the Kubernetes based LTB, it should provide a high level overview of the system.</p>"},{"location":"architecture/k8s-ltb-architecture/#system-context-diagram","title":"System Context Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#container-diagram","title":"Container Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#component-diagram","title":"Component Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#legend","title":"Legend","text":"<ul> <li>Dark blue: represents Personas (User, Admin)</li> <li>Blue: represents Internal Components (Frontend Web UI, LTB K8s Backend)</li> <li>Light blue: represents Components which will be implemented in this project (LTB Operator, LTB Operator API)</li> <li>Dark gray: represents External Components (K8s, Keycloak)</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/","title":"Preliminary Work KVM/Docker-based LTB Architecture","text":"<p>The Kubernetes based LTB was inspired by a previous implementation of the LTB, which was based on direct KVM and Docker usage.</p> <p>The following diagram shows the components of the KVM/Docker-based LTB, the lines indicate communication between the components:</p> <p></p> <p>Currently the KVM/Docker-based LTB is composed of the following containers:</p> <ul> <li>Frontend built with React</li> <li>Backend built with Django</li> <li>Databases (PostgreSQL, Redis)</li> <li>Beat</li> <li>Celery</li> <li>Web-SSH</li> <li>Prometheus</li> <li>Traefik</li> <li>Nginx</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#backend","title":"Backend","text":"<p>The backend is accessible via API and an Admin Web UI. It is responsible for the following tasks:</p> <ul> <li>parsing the yaml topology files</li> <li>deploying/destroying the containers and vms</li> <li>exposes status of lab deployments</li> <li>exposes information on how to access the deployed containers and vms</li> <li>provides remote ssh capabilities</li> <li>provides remote Wireshark capture capabilities</li> <li>managing reservations (create, delete, etc.)</li> <li>exposes node resource usage</li> <li>user management</li> <li>exposes information about a device (version, groups, etc.)</li> </ul> <p>It is composed of the following components:</p> <ul> <li>Reservations</li> <li>Running lab store</li> <li>Template store</li> <li>Authentication</li> </ul> <p>The orchestration component is responsible for creating different tasks using Celery and executing them on a remote host. There are 4 different types of tasks:</p> <ul> <li>DeploymentTask</li> <li>Deploys containers in docker</li> <li>Deploys VMs using KVM</li> <li>Creates connections between containers and VMs using an OVS bridge</li> <li>RemovalTask</li> <li>Removes a running lab</li> <li>MirrorInterfaceTask</li> <li>Creates a mirror interface on a connection</li> <li>SnapshotTask</li> <li>Takes a snapshot of a running lab</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#reservations","title":"Reservations","text":"<p>The reservation component is responsible for reserving system resources in advance. It is responsible for the following tasks:</p> <ul> <li>Create a reservation</li> <li>Delete a reservation</li> <li>Update a reservation</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#running-lab-store","title":"Running lab store","text":"<p>This component is responsible for storing information about running labs, such as:</p> <ul> <li>The devices taking part in the running lab, including the interfaces</li> <li>Connection information</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#template-store","title":"Template store","text":"<p>This component is responsible for storing lab templates.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#authentication","title":"Authentication","text":"<p>This component is responsible for user authentication and management.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#databases","title":"Databases","text":"<p>The following databases are used:</p> <ul> <li>PostgreSQL</li> <li>Redis for caching</li> </ul> <p>The Databases are used by the following components:</p> <ul> <li>Backend</li> <li>Beat</li> <li>Celery</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#beat","title":"Beat","text":"<p>The beat component is responsible for scheduling periodic tasks. To be more precise, it is responsible for scheduling the deployment and deletion of labs, according to the reservation information.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#celery","title":"Celery","text":"<p>Celery is used to execute the commands to create and delete the lab nodes and connections.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#web-ssh","title":"Web-SSH","text":"<p>Web-SSH is used to provide a web based ssh client, which can be used to access the deployed containers and vms aka lab nodes.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#prometheus","title":"Prometheus","text":"<p>Prometheus is used to collect metrics about CPU, memory and disk usage of the hypervisor nodes.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#traefik","title":"Traefik","text":"<p>Traefik is used as a proxy for the following components:</p> <ul> <li>Frontend</li> <li>Web-SSH</li> <li>Prometheus</li> <li>Nginx</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#nginx","title":"Nginx","text":"<p>Nginx is used as a reverse proxy for the backend.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#frontend","title":"Frontend","text":"<p>The frontend provides a web UI with the following features:</p> <ul> <li>User authentication</li> <li>Management of lab templates</li> <li>Management of reservations for labs</li> <li>Start/stop running labs</li> <li>Resource usage overview</li> <li>Provides information on how to access the deployed containers and vms</li> <li>Create wireshark capture interfaces</li> </ul>"},{"location":"contributions/ci/","title":"Continuos Integration","text":"<p>We use GitHub Actions as our CI/CD tool. Currently, we have two workflows:</p> <ul> <li>Deploy docs</li> <li>Operator CI</li> </ul>"},{"location":"contributions/ci/#deploy-docs","title":"Deploy docs","text":"<p>This workflow is used to deploy the mkdocs documentation to GitHub Pages. It is triggered on every push to the <code>main</code> branch, that affects the documentation. Specifically, it is triggered when a file in the <code>docs</code> directory or the <code>mkdocs.yaml</code> configuration file has changed.</p> <p>Check the deploy-docs-ci.yaml action for more details.</p>"},{"location":"contributions/ci/#operator-ci","title":"Operator CI","text":"<p>This workflow is used to test, build and push the LTB Operator image, CatalogSource and Bundle to the GitHub Container Registry. It is triggered for every push to a pull request and for every push to the <code>main</code> branch.</p> <p>You can find this action here: operator-ci.yaml</p>"},{"location":"contributions/ci/#test","title":"Test","text":"<p>The test step runs the unit tests of the LTB Operator and fails the pipeline if any test fails. Additionally, a coverage report is generated and uploaded to Codecov. Pull requests are checked for the code coverage and will block the merge if the coverage drops below 80%. This is ensured by the Codecov GitHub integration and defined in the codecov.yml file.</p>"},{"location":"contributions/ci/#build","title":"Build","text":"<p>We use the GitHub actions provided by Docker to build and push the LTB Operator image to the GitHub Container Registry. Additionally, we use cosign to sign the images, so that users can verify the authenticity of the image.</p>"},{"location":"contributions/ci/#additional-deployment-artifacts","title":"Additional deployment artifacts","text":"<p>To be able to deploy the operator with the Operator Lifecycle Manager, a Bundle and a CatalogSource must be created. These artifacts are created with the [Operator-SDK], to simplify the pipeline these tasks have been exported to the Makefile</p>"},{"location":"contributions/coding-conventions/","title":"Coding Conventions","text":"<p>We are following the Effective Go guidelines for coding conventions. The following is a summary of the most important conventions.</p>"},{"location":"contributions/coding-conventions/#naming","title":"Naming","text":"<p>The following naming conventions are used in the project:</p>"},{"location":"contributions/coding-conventions/#naming-conventions-in-go","title":"Naming conventions in Go","text":"<ul> <li>camelCase for variables and functions, which are not exported</li> <li>PascalCase for types and functions that need to be exported</li> </ul>"},{"location":"contributions/coding-conventions/#examples","title":"Examples","text":"<ul> <li>labInstanceStatus: variable name for a status of a lab instance</li> <li>UpdateLabInstanceStatus: name for an exported function, starts with a capital letter</li> </ul>"},{"location":"contributions/coding-conventions/#formatting","title":"Formatting","text":"<p>We are using the gofmt from the Go standard library to format our code.</p> <p>staticcheck is used as a linter in addition to the formatting guidelines from Effective Go, because it is the default linter of the Go extension for VS Code.</p>"},{"location":"contributions/contributor-guide/","title":"Contributor Guide","text":"<p>Contributions are welcome and appreciated.</p>"},{"location":"contributions/contributor-guide/#how-it-works","title":"How it works","text":"<p>This project aims to follow the Kubernetes Operator pattern</p> <p>It uses Controllers, which provides a reconcile function responsible for synchronizing resources until the desired state is reached on the cluster.</p>"},{"location":"contributions/contributor-guide/#development-environment","title":"Development Environment","text":"<p>You\u2019ll need a Kubernetes cluster to run against. You can find instructions on how to setup your dev cluster in the Dev Cluster Setup section. Note: Your controller will automatically use the current context in your kubeconfig file (i.e. whatever cluster <code>kubectl cluster-info</code> shows).</p> <p>We recommend using VSCode with the Remote - Containers extension. This will allow you to use our devcontainer, which has all the tools needed to develop and test the operator already installed.</p>"},{"location":"contributions/contributor-guide/#prerequisites-for-recommended-ide-setup","title":"Prerequisites for recommended IDE setup","text":"<ul> <li>Docker</li> <li>VSCode</li> <li>Remote - Containers extension</li> </ul>"},{"location":"contributions/contributor-guide/#getting-started","title":"Getting Started","text":"<ol> <li>Clone the repository</li> <li>Open the repository in VSCode</li> <li>Click the popup or use the command palette to reopen the repository in a container (Dev Containers: Reopen in Container)</li> </ol> <p>Now you are ready to start developing!</p>"},{"location":"contributions/contributor-guide/#running-the-operator-locally","title":"Running the operator locally","text":"<p>You can run the operator locally on your machine, this is useful for quick testing and debugging. However, you will need to be aware that the operator will use the kubeconfig file on your machine, so you will need to make sure that the context is set to the cluster you want to run against. Therefore it also does not use the RBAC rules it would usually be deployed with.</p> <ol> <li>Install the CRDs into the cluster:</li> </ol> <pre><code>make install\n</code></pre> <ol> <li>Run your controller (this will run in the foreground, so switch to a new terminal if you want to leave it running):</li> </ol> <pre><code>make run\n</code></pre> <p>NOTE: You can also run this in one step by running: <code>make install run</code></p>"},{"location":"contributions/contributor-guide/#uninstall-crds","title":"Uninstall CRDs","text":"<p>To delete the CRDs from the cluster:</p> <pre><code>make uninstall\n</code></pre>"},{"location":"contributions/contributor-guide/#running-the-operator-on-the-cluster","title":"Running the operator on the cluster","text":"<p>You can also run the operator on the cluster, this is useful for testing the operator in a more realistic environment. However, you will first need to login to some container registry that the cluster can access, so that you can push the operator image to that registry. This will allow you to test the operators RBAC rules.</p> <p>Make sure to replace <code>&lt;some-registry&gt;</code> with the location of your container registry and <code>&lt;tag&gt;</code> with the tag you want to use.</p> <ol> <li>Install Instances of Custom Resources:</li> </ol> <pre><code>kubectl apply -f config/samples/\n</code></pre> <ol> <li>Build and push your image to the location specified by <code>IMG</code>:</li> </ol> <pre><code>make docker-build docker-push IMG=&lt;some-registry&gt;/ltb-operator:&lt;tag&gt;\n</code></pre> <ol> <li>Deploy the controller to the cluster with the image specified by <code>IMG</code>:</li> </ol> <pre><code>make deploy IMG=&lt;some-registry&gt;/ltb-operator:&lt;tag&gt;\n</code></pre>"},{"location":"contributions/contributor-guide/#undeploy-controller","title":"Undeploy controller","text":"<p>Undeploy the controller from the cluster:</p> <pre><code>make undeploy\n</code></pre>"},{"location":"contributions/contributor-guide/#modifying-the-api-definitions","title":"Modifying the API definitions","text":"<p>If you are editing the API definitions, generate the manifests such as CRs or CRDs using:</p> <pre><code>make manifests\n</code></pre> <p>NOTE: Run <code>make --help</code> for more information on all potential <code>make</code> targets</p> <p>You can find more information on how to develop the operator in the Operator SDK Documentation and the Kubebuilder Documentation</p>"},{"location":"contributions/dev-cluster-setup/","title":"Development cluster setup","text":"<p>Steps to setup a Kubernetes development cluster for testing and development of the LTB Operator.</p>"},{"location":"contributions/dev-cluster-setup/#prerequisites-remote-cluster","title":"Prerequisites Remote Cluster","text":"<ul> <li>Server with Linux OS (Recommended Ubuntu 22.04)</li> </ul>"},{"location":"contributions/dev-cluster-setup/#prepare-node","title":"Prepare Node","text":"<pre><code>sudo apt update\nsudo apt upgrade -y\nsudo swapoff -a\nsudo sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#rke2-server-configuration","title":"RKE2 Server Configuration","text":"<pre><code>sudo mkdir -p /etc/rancher/rke2\nsudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <pre><code># /etc/rancher/rke2/config.yaml\nwrite-kubeconfig-mode: \"0644\"\nkube-apiserver-arg: \"allow-privileged=true\"\ncni: multus,cilium\ndisable-kube-proxy: true\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#cilium-configuration-for-multus","title":"Cilium Configuration for Multus","text":"<pre><code>sudo mkdir -p /var/lib/rancher/rke2/server/manifests\nsudo vim /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml\n</code></pre> <pre><code># /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml\n# k8sServiceHost/Port IP of Control Plane node default Port 6443\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\nname: rke2-cilium\nnamespace: kube-system\nspec:\nvaluesContent: |-\ncni:\nchainingMode: \"none\"\nexclusive: false\nkubeProxyReplacement: strict\nk8sServiceHost: \"&lt;NodeIP&gt;\"\nk8sServicePort: 6443\noperator:\nreplicas: 1\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-and-start-server-and-check-logs","title":"Install and start Server and check logs","text":"<pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=v1.26.0+rke2r2 sudo -E sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\nsudo journalctl -u rke2-server -f\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#add-kubernetes-tools-to-path-and-set-kubeconfig","title":"Add Kubernetes tools to path and set kubeconfig","text":"<p>Adds kubectl, crictl and ctr to path</p> <pre><code>echo 'export PATH=\"$PATH:/var/lib/rancher/rke2/bin\"' &gt;&gt; ~/.bashrc\necho 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bashrc\necho 'alias k=kubectl' &gt;&gt; ~/.bashrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt;~/.bashrc\nsource ~/.bashrc\nmkdir ~/.kube\nln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#get-token-for-agent","title":"Get Token for Agent","text":"<pre><code>sudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#rke2-agent-configuration-optional","title":"RKE2 Agent Configuration (Optional)","text":"<pre><code>sudo mkdir -p /etc/rancher/rke2\nsudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <pre><code># /etc/rancher/rke2/config.yaml\n---\nserver: https://&lt;server&gt;:9345\ntoken: &lt;token from server node&gt;\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-and-start-agent-and-check-logs","title":"Install and start Agent and check logs","text":"<pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" INSTALL_RKE2_VERSION=v1.26.0+rke2r2 sudo -E sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\nsudo journalctl -u rke2-agent -f\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-cluster-network-addons-operator","title":"Install Cluster Network Addons Operator","text":"<p>The Cluster Network Addons Operator can be used to deploy additional networking components. Multus and Cilium are already installed via RKE2. Open vSwitch CNI Plugin can be installed via this operator.</p> <p>First install the operator itself:</p> <pre><code>kubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/namespace.yaml\nkubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/network-addons-config.crd.yaml\nkubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/operator.yaml\n</code></pre> <p>Then you need to create a configuration for the operator example CR:</p> <pre><code>kubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/network-addons-config-example.cr.yaml\n</code></pre> <p>Wait until the operator has finished the installation:</p> <pre><code>kubectl wait networkaddonsconfig cluster --for condition=Available\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#kubevirt","title":"Kubevirt","text":"<p>Kubevirt is a Kubernetes add-on to run virtual machines.</p>"},{"location":"contributions/dev-cluster-setup/#validate-hardware-virtualization-support","title":"Validate Hardware Virtualization Support","text":"<pre><code>sudo apt install libvirt-clients\nsudo virt-host-validate qemu\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-kubevirt","title":"Install Kubevirt","text":"<p>Latest Release: <code>export RELEASE=$(curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt)</code></p> <pre><code>export RELEASE=v0.58.1\n# Deploy the KubeVirt operator\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml\n# Create the KubeVirt CR (instance deployment request) which triggers the actual installation\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml\n# wait until all KubeVirt components are up\nkubectl -n kubevirt wait kv kubevirt --for condition=Available\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-containerized-data-importer","title":"Install Containerized Data Importer","text":"<pre><code>export CDI_VERSION=v1.55.2\nkubectl create ns cdi\nkubectl -n cdi apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml\nkubectl -n cdi apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#install-virtctl-via-krew","title":"Install virtctl via Krew","text":"<p>First install Krew and then install virtctl via Krew</p> <pre><code>(\nset -x; cd \"$(mktemp -d)\" &amp;&amp;\nOS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\nARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\nKREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\ncurl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\ntar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n./\"${KREW}\" install krew\n)\necho 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nkubectl krew install virt\nkubectl virt help\n</code></pre> <p>You are ready to go!</p> <p>You're now ready to use the cluster for development or testing purposes.</p>"},{"location":"contributions/dev-cluster-setup/#metallb","title":"MetalLB","text":"<p>You can optionally install MetalLB, currently it is not required to use the LTB Operator. MetalLB is a load-balancer implementation for bare metal Kubernetes clusters.</p>"},{"location":"contributions/dev-cluster-setup/#install-operator-lifecycle-manager-olm","title":"Install Operator Lifecycle Manager (OLM)","text":"<p>Install Operator Lifecycle Manager (OLM), a tool to help manage the operators running on your cluster.</p> <pre><code>curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.24.0/install.sh | bash -s v0.24.0\n</code></pre> <p>Install the operator by running the following command:</p> <pre><code>kubectl create -f https://operatorhub.io/install/metallb-operator.yaml\n</code></pre> <p>This operator will be installed in the \"operators\" namespace and will be usable from all namespaces in the cluster.</p> <p>After install, watch your operator come up using next command.</p> <pre><code>kubectl get csv -n operators\n</code></pre> <p>Now create a MetalLB IPAddressPool CR to configure the IP address range that MetalLB will use:</p> <pre><code>sudo vim metallb-ipaddresspool.yaml\n</code></pre> <pre><code># metallb-ipaddresspool.yaml\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\nname: default\nnamespace: operators\nspec:\naddresses:\n- X.X.X.X/XX\n</code></pre> <p>Create a L2Advertisement to tell MetalLB to responde to ARP requests for all IP address pools (no named ip address pool, means all pools):</p> <pre><code>sudo vim l2advertisment.yaml\n</code></pre> <pre><code># l2advertisment.yaml\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\nname: default\nnamespace: operators\nspec:\nipAddressPools:\n- default\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f metallb-ipaddresspool.yaml\nkubectl apply -f l2advertisment.yaml\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#storage","title":"Storage","text":"<p>To store your virtual machine images and disks you may want to use a storage backend. Currently no storage backend has been tested with the LTB Operator, but you can try to use Trident. Trident is a dynamic storage provisioner for Kubernetes, it supports many storage backends, including NetApp, AWS, Azure, Google Cloud, and many more.</p> <p>Following you will find some instructions that may help you to install Trident on your cluster.</p> <p>You always can find more information about Trident in the official documentation.</p> <p>Check connectivity to NetApp Storage:</p> <pre><code>kubectl run -i --tty ping --image=busybox --restart=Never --rm -- \\\nping &lt;NetApp Management IP&gt;\n</code></pre> <p>Download and extract the Trident installer:</p> <pre><code>export TRIDENT_VERSION=23.01.0\nwget https://github.com/NetApp/trident/releases/download/v$TRIDENT_VERSION/trident-installer-$TRIDENT_VERSION.tar.gz\ntar -xf trident-installer-$TRIDENT_VERSION.tar.gz\ncd trident-installer\nmkdir setup\nvim ./setup/backend.json\n</code></pre> <p>Configure the installer:</p> <pre><code># ./backend.json\n{\n\"version\": 1,\n    \"storageDriverName\": \"ontap-nas\",\n    \"managementLIF\": \"&lt;NetApp Management IP&gt;\",\n    \"dataLIF\": \"&lt;NetApp Data IP&gt;\",\n    \"svm\": \"svm_k8s\",\n    \"username\": \"admin\",\n    \"password\": \"&lt;NetApp Password&gt;\",\n    \"storagePrefix\": \"trident_\",\n    \"nfsMountOptions\": \"-o nfsvers=4.1 -o mountport=2049 -o nolock\",\n    \"debug\": true\n}\n</code></pre> <p>Install Trident:</p> <pre><code>./tridentctl install -n trident -f ./setup/backend.json\n</code></pre> <p>Check the installation:</p> <pre><code>kubectl get pods -n trident\n</code></pre>"},{"location":"contributions/dev-cluster-setup/#local-development-cluster","title":"Local development cluster","text":"<p>K3d, Minikube or Kind can be used to run a local Kubernetes cluster, if you don't have access to a remote cluster/server.</p> <p>Make sure to install the following tools:</p> <ul> <li>KubeVirt</li> <li>Containerized Data Importer</li> <li>Cluster Network Addon Operator</li> </ul> <p>KubeVirt may not work properly on local development clusters</p> <p>KubeVirt may not work properly on local development clusters, because it requires nested virtualization support, which is not available on all local development clusters. Make sure to enable nested virtualization on your local machine, if you want to run KubeVirt on a local development cluster.</p>"},{"location":"contributions/technologies-used/","title":"Tools and Frameworks","text":"<p>The tools and frameworks used in the project are listed below.</p>"},{"location":"contributions/technologies-used/#go-based-operator-sdk-framework","title":"Go-based Operator SDK framework","text":"<p>To create the LTB Operator, we used the Go-based Operator-SDK framework, which provides a set of tools to simplify the process of building, testing and packaging our operator.</p>"},{"location":"contributions/technologies-used/#kubevirt","title":"Kubevirt","text":"<p>Kubevirt is a tool that provides a virtual machine management layer on top of Kubernetes. It allows us to deploy virtual machines on Kubernetes.</p>"},{"location":"contributions/technologies-used/#kubernetes","title":"Kubernetes","text":"<p>We use Kubernetes as the container orchestration platform for the LTB application.</p>"},{"location":"contributions/technologies-used/#multus-cni","title":"Multus CNI","text":"<p>To create multiple network interfaces for the pods, Multus CNI is used.</p>"},{"location":"contributions/test-concepts/","title":"Test Concept","text":"<p>This document outlines the approaches, methodologies, and types of tests that ensure that the LTB Operator components are functioning as expected.</p>"},{"location":"contributions/test-concepts/#test-categories","title":"Test categories","text":"<p>The tests primarily focus on Functionality and Logic. Security and Performance tests should be added in the future as the project matures.</p>"},{"location":"contributions/test-concepts/#tools","title":"Tools","text":"<p>The following tools are used to test the LTB Operator, you can find more information why they were chosen in the Testing Framework Decision:</p> <ul> <li>Testing: The default Go testing library that provides support for automated testing of Go packages. It is intended to be used in concert with the \"go test\" command, which automates execution of any function of the form</li> <li>Ginkgo: a Go testing framework for Go to help you write expressive, readable, and maintainable tests. It is best used with the Gomega matcher library.</li> <li>Gomega: a Go matcher library that provides a set of matchers to perform assertions in tests. It is best used with the Ginkgo testing framework.</li> </ul>"},{"location":"contributions/test-concepts/#strategies-test-approach","title":"Strategies: Test Approach","text":"<p>We focused on unit tests for the LTB Operator, as they were easy to implement and provide a good coverage of the code. At a later stage of the project, we could add integration tests to ensure that the LTB Operator works as expected with other components like the LTB API.</p> <p>We aspire to achieve approximately 90% test coverage, to increase the maintainability and stability of the LTB Operator.</p>"},{"location":"contributions/test-concepts/#unit-tests","title":"Unit Tests","text":"<p>Unit rely on the Fake Client package from the controller-runtime library to create a fake client that can be used to mock interactions with the Kubernetes API. This allows us to test functions that interact with the Kubernetes API without mocking the complete API or using a real Kubernetes cluster.</p>"},{"location":"contributions/test-concepts/#integration-tests","title":"Integration Tests","text":"<p>The integration tests could be implemented by using the EnvTest package from the controller-runtime library.</p>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/","title":"Use Markdown Architectural Decision Records","text":""},{"location":"decisions/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record design decisions made in this project. Which format and structure should these records follow?</p>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR 2.1.2 \u2013 The Markdown Architectural Decision Records</li> <li>Michael Nygard's template \u2013 The first incarnation of the term \"ADR\"</li> <li>Sustainable Architectural Decisions \u2013 The Y-Statements</li> <li>Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 2.1.2\", because</p> <ul> <li>Implicit assumptions should be made explicit.   Design documentation is important to enable people understanding the decisions later on.   See also A rational design process: How and why to fake it.</li> <li>The MADR format is lean and fits our development style.</li> <li>The MADR structure is comprehensible and facilitates usage &amp; maintenance.</li> <li>The MADR project is vivid.</li> <li>Version 2.1.2 is the latest one available when starting to document ADRs.</li> <li>A Visual Studio Code extension ADR Manager for MADR exits, which makes managing ADRs easy.</li> </ul>"},{"location":"decisions/0001-operator-SDK/","title":"Operator SDK","text":""},{"location":"decisions/0001-operator-SDK/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>It's best practice to use an SDK to build operators for Kubernetes. The SDK provides a higher level of abstraction for creating Kubernetes operators, which makes it easier to write and manage operators. There are multiple SDKs available for building operators. We need a SDK that's flexible and easy to use and can be used with Go.</p>"},{"location":"decisions/0001-operator-SDK/#considered-options","title":"Considered Options","text":"<ul> <li>Operator SDK (Operator Framework)</li> <li>KubeBuilder</li> <li>Kopf</li> <li>KUDO</li> <li>Metacontroller</li> </ul>"},{"location":"decisions/0001-operator-SDK/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Operator SDK\", because it provides a high level of abstraction for creating Kubernetes operators, which makes it easier to write and manage operators. Additionally, there are tools and libraries for building and testing the operator included in Operator SDK, it's easy to use and can be used with Go.</p>"},{"location":"decisions/0001-operator-SDK/#links","title":"Links","text":"<ul> <li>Operator SDK</li> <li>Tools to build an operator</li> </ul>"},{"location":"decisions/00011-remote-access/","title":"Remote-Access","text":""},{"location":"decisions/00011-remote-access/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>For the lab instances to be useful for the students, they need to be able to access the pods (containers) and VMs. Access to pods/VMs should only be granted to user with the appropriate access rights. It should be possible to access the pods/VMs console and or access it via multiple OOB protocols (SSH, RDP, VNC, etc.).</p>"},{"location":"decisions/00011-remote-access/#considered-options","title":"Considered Options","text":"<ul> <li>Kubernetes Service</li> <li>Gotty</li> <li>ttyd</li> </ul>"},{"location":"decisions/00011-remote-access/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"ttyd and Kubernetes Service\", ttyd will be used as a jump host to access the pods/VMs console. A Kubernetes service will be used to allow access the pods/VMs via OOB protocols. Security for the console access will likely be easy to implement. Secure access via OOB protocols was considered, but will need to be researched further, currently it would depend on the OOB protocol used and the security features it provides.</p>"},{"location":"decisions/0002-operator-scope/","title":"Operator Scope","text":""},{"location":"decisions/0002-operator-scope/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The Operator could be a namespace-scoped or cluster-scoped.</p>"},{"location":"decisions/0002-operator-scope/#considered-options","title":"Considered Options","text":"<ul> <li>Namespace-scoped</li> <li>Cluster-scoped</li> </ul>"},{"location":"decisions/0002-operator-scope/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Cluster-scoped\", because cluster-scoped operators enable you to manage namespaces or resources in the entire cluster. This is needed to ensure that each lab instance can be deployed within its own namespace. Cluster-scoped operators are also capable of managing infrastructure-level resources, such as nodes. Additionally, cluster-scoped operators provide greater visibility and control over the entire cluster.</p>"},{"location":"decisions/0002-operator-scope/#links","title":"Links","text":"<ul> <li>Operator Scope</li> </ul>"},{"location":"decisions/0003-api-and-operator-deployment/","title":"API and Operator Deployment","text":""},{"location":"decisions/0003-api-and-operator-deployment/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The operator and the API could be separated and deployed as two services/containers or they could be deployed as one service/container.</p>"},{"location":"decisions/0003-api-and-operator-deployment/#considered-options","title":"Considered Options","text":"<ul> <li>One container</li> <li>Separate containers</li> </ul>"},{"location":"decisions/0003-api-and-operator-deployment/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Separate containers\", because it provides more flexibility and scalability. It also makes it easier to update the operator and the API separately. Additionally, it is easy to separate the API and the operator into two different services, as they talk to each other via the Kubernetes API.</p>"},{"location":"decisions/0004-dev-container/","title":"Dev Container","text":""},{"location":"decisions/0004-dev-container/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Every team member could set up their development environment manually or we could use a dev container to provide a consistent development environment for all team members and future contributors.</p>"},{"location":"decisions/0004-dev-container/#considered-options","title":"Considered Options","text":"<ul> <li>Dev Container</li> <li>Manual Setup</li> </ul>"},{"location":"decisions/0004-dev-container/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Dev Container\", because a dev container setup lets you create the same development environment for all team members, which ensures consistency. It also provides a completely isolated development environment, which helps to avoid software incompatibility issues, such as operator-sdk not working on Windows. Moreover, a dev container is easily portable and works on all operating systems that support Docker. The only downside is that not all IDEs support dev containers, but at least two of the currently most popular IDEs VS Code and Visual Studio support dev containers.</p>"},{"location":"decisions/0004-dev-container/#links","title":"Links","text":"<ul> <li>DevContainer</li> <li>Most Popular IDEs</li> </ul>"},{"location":"decisions/0005-replace-current-ltb-backend/","title":"Replace Current LTB Backend","text":""},{"location":"decisions/0005-replace-current-ltb-backend/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The LTB K8s Backend could replace the current LTB Backend fully or partially by leaving features, such as User-Management in the current LTB Backend.</p>"},{"location":"decisions/0005-replace-current-ltb-backend/#considered-options","title":"Considered Options","text":"<ul> <li>Replace current LTB Backend fully</li> <li>Replace current LTB Backend partially</li> </ul>"},{"location":"decisions/0005-replace-current-ltb-backend/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Replace current LTB Backend fully\", because huge parts of the current LTB Backend would need to be rewritten to be compatible with the new LTB K8s operator and it would be easier to rewrite the whole backend. Additionally, the same programming language can be used throughout the whole Backend, Go.</p>"},{"location":"decisions/0006-lab-instance-set/","title":"Lab Instance Set","text":""},{"location":"decisions/0006-lab-instance-set/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We could create a CR called LabInstanceSet, and tell the operator we want e.g. 10 LabInstances by providing one LabInstance CR and a generator like a list of names or we could provide the operator 10 CRs to create 10 LabInstances.</p>"},{"location":"decisions/0006-lab-instance-set/#considered-options","title":"Considered Options","text":"<ul> <li>With LabInstanceSet</li> <li>Without LabInstanceSet</li> </ul>"},{"location":"decisions/0006-lab-instance-set/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Without LabInstanceSet\", because we currently don't see a need for it. This could change in the future, but for now we will not implement it.</p>"},{"location":"decisions/0007-interaction-with-operator/","title":"Interaction with Operator","text":""},{"location":"decisions/0007-interaction-with-operator/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>There are multiple ways to interact with the operator, such as a GitOps approach, using the frontend via the API or using <code>kubectl</code>.</p>"},{"location":"decisions/0007-interaction-with-operator/#considered-options","title":"Considered Options","text":"<ul> <li>GitOps</li> <li>Use frontend</li> <li>Use kubectl</li> <li>All</li> </ul>"},{"location":"decisions/0007-interaction-with-operator/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"All\", because these options are not mutually exclusive and can be used together, we want to support all of them.</p>"},{"location":"decisions/0008-namespace-per-labinstance/","title":"Namespace Per LabInstance","text":""},{"location":"decisions/0008-namespace-per-labinstance/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>LabInstances could be created in separate namespaces or one namespace for all LabInstances.</p>"},{"location":"decisions/0008-namespace-per-labinstance/#considered-options","title":"Considered Options","text":"<ul> <li>One Namespace for all LabInstances</li> <li>Namespace per LabInstance</li> </ul>"},{"location":"decisions/0008-namespace-per-labinstance/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Namespace per LabInstance\", because it will be easier in the future to implement features like RBAC and resource quotas and limits.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/","title":"K8s Aggreted API Over Standalone API","text":""},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want a declarative way of creating LTB labs inside Kubernetes using Kubernetes native pods and KubeVirt virtual machines. We could either create a standalone API which interacts with the Kubernetes API and does not follow the Kubernetes API conventions. And therefore is not compatible with Kubernetes tools, such as dashboards or <code>kubectl</code>, but would allow complete control over the API design. Or we could create an aggregated API which uses the Kubernetes aggregation layer to extend the Kubernetes API, which would allow us to use Kubernetes tools, such as dashboards or <code>kubectl</code>, but would limit the control over the API design.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#considered-options","title":"Considered Options","text":"<ul> <li>Standalone API</li> <li>Aggregated API</li> </ul>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Aggregated API\", because Is better suited for declerative API, our new types will be readable and writeable using kubectl/Kubernetes tools, such as dashboards. We also can leverage Kuberbetes API support features this way. Additionally, our resources are scoped to a cluster or namespaces of a cluster. Finally, the Operator Pattern is simpler to implement this way.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#links","title":"Links","text":"<ul> <li>Standalone vs Aggregated API</li> <li>Aggregated API</li> <li>Operator Pattern</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/","title":"Extending LTB with New Node Types","text":""},{"location":"decisions/0010-extending-ltb-with-new-node-types/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>It should be possible to create, update and delete node types (e.g. Ubuntu, XRD, XR, IOS, Cumulus, etc.) Node types should be used inside lab templates and expose a way to provide a node with configuration (cloud-init, zero-touch, etc.) The amount of available network interfaces is dynamic and depends on how many connections a node has according to a specific lab template.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Certain operating systems' images like XR, and XRD need a specific interface configuration which depends on how many interfaces a certain node will receive.</li> <li>The chosen solution should support multiple version of a type in an easy to use way (e.g. Ubuntu 22.04, 20.04, ...).</li> <li>For XRd images, interfaces need to have environment variables set for each interface they use, and the interface count needs to be dynamically set according to the lab template.</li> <li>For XR VM images, the first interface is the management interface and then there are two empty interfaces that need a special configuration.</li> <li>For mount from config might be different</li> <li>Cumulus VX images need a privileged container</li> <li>XRd need additional privileges</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#considered-options","title":"Considered Options","text":"<ul> <li>Custom Resources</li> <li>Go</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Custom Resources\", because it will be possible to support all the cases mentioned in the decision drivers using go templates and CRs. Implementing the types in Go does not seem to bring any major advantages, whereas using CRs will be easier for external users to extend the system with new node types.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Easy to extend during runtime</li> <li>Easy to extend for external users</li> <li>All decision drivers will be supported</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Go Templates are not as powerful as Go, which could make it harder to implement certain node types.</li> <li>A Custom Resource is also a little bit less flexible than a Go type, but this should not be a problem for the use cases we have.</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#links","title":"Links","text":"<ul> <li>Example of similar solution using Go node types</li> </ul>"},{"location":"decisions/0011-programming-language/","title":"Programming Language","text":""},{"location":"decisions/0011-programming-language/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need to choose a programming language for the project. The considered options are based on the supported languages of the Operator SDK.</p>"},{"location":"decisions/0011-programming-language/#considered-options","title":"Considered Options","text":"<ul> <li>Go</li> <li>Helm</li> <li>Ansible</li> </ul>"},{"location":"decisions/0011-programming-language/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Go\", because many cloud native projects are written in Go, and it is a compiled language, which is more performant than interpreted languages. Also, Go is a statically typed language, which makes it easier to maintain and refactor the code. It is also easier to write complicated logic and tests in Go than in Helm or Ansible.</p>"},{"location":"decisions/0012-testing-framework/","title":"Testing Framework","text":""},{"location":"decisions/0012-testing-framework/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Every project needs to be tested. There are multiple testing libraries and frameworks to test Go applications, which can be used in addition to the default Go testing library.</p>"},{"location":"decisions/0012-testing-framework/#considered-options","title":"Considered Options","text":"<ul> <li>Testify</li> <li>Ginkgo/Gomega</li> <li>GoSpec</li> <li>GoConvey</li> </ul>"},{"location":"decisions/0012-testing-framework/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Ginkgo/Gomega\", because it is widely used in the Kubernetes community to test Kubernetes operators. It is also used in the Kubevirt project, which is used in the LTB Operator. Additionally, tests written with Ginkgo/Gomega are easy to read and understand.</p>"}]}