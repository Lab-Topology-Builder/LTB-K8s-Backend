{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lab Topology Builder Documentation","text":"<p>The Lab Topology Builder (LTB) is an open source project that allows users to deploy networking labs on Kubernetes. It is a tool that enables you to build a topology of virtual machines and containers, which are connected to each other according to the network topology you have defined.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Deployment of networking labs on Kubernetes</li> <li>Deletion of networking labs on Kubernetes</li> <li>Status querying of lab deployments</li> <li>Remote access to OOB management of lab nodes (e.g. SSH)</li> </ul> <p>Upcoming features:</p> <ul> <li>Managing custom node types via our Kubernetes Resource</li> <li>Secure remote access to OOB management of lab nodes (Access Control)</li> <li>User management</li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#lab-topology-builder-ltb","title":"Lab Topology Builder (LTB)","text":"<p>The Lab Topology Builder (LTB) is a tool that allows you to build a topology of virtual machines and containers, which are connected to each other according to the network topology you have defined.</p> <p></p>"},{"location":"concepts/#ltb-kubernetes-backend","title":"LTB Kubernetes Backend","text":"<p>The LTB Kubernetes Backend consists of the LTB Operator and the LTB Operator API, which replaces the current (KVM/Docker) based backend of LTB. It is responsible for managing the lab instances in a Kubernetes cluster and provide a REST API for the frontend to interact with the kubernetes cluster.</p>"},{"location":"concepts/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>Kubernetes Cluster is a set of nodes that run containerized applications managed by Kubernetes. A Kubernetes cluster consists of a control plane and one or more nodes. The control plane is responsible for maintaining the desired state of the cluster, such as which applications are running and which container images they use, etc. And the nodes run the applications and workloads.</p>"},{"location":"concepts/#kubernetes-operator","title":"Kubernetes operator","text":"<p>A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts but includes domain or application-specific knowledge to automate the entire life cycle of the software it manages.</p>"},{"location":"concepts/#ltb-kubernetes-operator","title":"LTB Kubernetes Operator","text":"<p>The LTB Kubernetes Operator is a K8s Operator for the LTB application, which manages lab templates, lab instances in a Kubernetes cluster. It manages lab instances, node types, etc. It also updates the status of the lab instances according to the current state of the containers and virtual machines that are part of the lab instance.</p>"},{"location":"concepts/#custom-resource-definition-crd","title":"Custom Resource Definition (CRD)","text":"<p>A custom resource definition (CRD) is a Kubernetes native resource. Defining a CRD object creates a new custom resource with a name and schema that you specify. The custom resource created from a CRD object can be either namespaced or cluster-scoped. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.</p> <p>The following example shows a CRD for a LabInstance object, how they are defined in go and how the YAML output looks like.</p> <pre><code>import (\nmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\ntype LabInstanceSpec struct {\nLabTemplateReference string `json:\"labTemplateReference\"`\n}\n\ntype LabInstanceStatus struct {\nStatus         string `json:\"status,omitempty\"`\nNumPodsRunning string `json:\"numpodsrunning,omitempty\"`\nNumVMsRunning  string `json:\"numvmsrunning,omitempty\"`\n}\n\n//+kubebuilder:object:root=true\n//+kubebuilder:subresource:status\n//+kubebuilder:printcolumn:name=\"STATUS\",type=string,JSONPath=`.status.status`\n//+kubebuilder:printcolumn:name=\"PODS_RUNNING\",type=string,JSONPath=`.status.numpodsrunning`\n//+kubebuilder:printcolumn:name=\"VMS_RUNNING\",type=string,JSONPath=`.status.numvmsrunning`\n\ntype LabInstance struct {\nmetav1.TypeMeta   `json:\",inline\"`\nmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\nSpec   LabInstanceSpec   `json:\"spec,omitempty\"`\nStatus LabInstanceStatus `json:\"status,omitempty\"`\n}\n\n// +kubebuilder:object:root=true\ntype LabInstanceList struct {\nmetav1.TypeMeta `json:\",inline\"`\nmetav1.ListMeta `json:\"metadata,omitempty\"`\nItems           []LabInstance `json:\"items\"`\n}\n</code></pre> <pre><code>---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\nannotations:\ncontroller-gen.kubebuilder.io/version: v0.10.0\ncreationTimestamp: null\nname: labinstances.ltb-backend.ltb\nspec:\ngroup: ltb-backend.ltb\nnames:\nkind: LabInstance\nlistKind: LabInstanceList\nplural: labinstances\nsingular: labinstance\nscope: Namespaced\nversions:\n- additionalPrinterColumns:\n- jsonPath: .status.status\nname: STATUS\ntype: string\n- jsonPath: .status.numpodsrunning\nname: PODS_RUNNING\ntype: string\n- jsonPath: .status.numvmsrunning\nname: VMS_RUNNING\ntype: string\nname: v1alpha1\nschema:\nopenAPIV3Schema:\nproperties:\napiVersion:\ndescription: 'APIVersion defines the versioned schema of this representation\nof an object. Servers should convert recognized schemas to the latest\ninternal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\ntype: string\nkind:\ndescription: 'Kind is a string value representing the REST resource this\nobject represents. Servers may infer this from the endpoint the client\nsubmits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\ntype: string\nmetadata:\ntype: object\nspec:\nproperties:\nlabTemplateReference:\ntype: string\nrequired:\n- labTemplateReference\ntype: object\nstatus:\nproperties:\nnumpodsrunning:\ntype: string\nnumvmsrunning:\ntype: string\nstatus:\ntype: string\ntype: object\ntype: object\nserved: true\nstorage: true\nsubresources:\nstatus: {}\n</code></pre>"},{"location":"concepts/#custom-resource-cr","title":"Custom Resource (CR)","text":"<p>A custom resource (CR) is an extension of the Kubernetes API that allows you to define and manage your own API objects. It provides a way to store and retrieve structured data and can be used with a custom controller to provide a declarative API. Custom resources can be defined as a Kubernetes API extension using Custom Resource Definitions (CRDs) or via API aggregation.</p>"},{"location":"concepts/#lab-template","title":"Lab Template","text":"<p>Lab Template is a CR, which defines a template for a lab. It contains information about the lab you want to deploy and a reference to the node types that are part of the lab.</p> <p>The following YAML is an example of a LabTemplate:</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabTemplate\nmetadata:\nlabels:\napp.kubernetes.io/name: labtemplate\napp.kubernetes.io/instance: labtemplate-sample\napp.kubernetes.io/part-of: operator\napp.kubernetes.io/managed-by: kustomize\napp.kubernetes.io/created-by: operator\nname: labtemplate-sample\nspec:\nnodes:\n- name: \"sample-node-1\"\nimage:\ntype: \"ubuntu\"\nversion: \"22.04\"\nkind: \"vm\"\nconfig: |-\n#cloud-config\npassword: ubuntu\nchpasswd: { expire: False }\nssh_authorized_keys:\n- &lt;your-ssh-pub-key&gt;\npackages:\n- qemu-guest-agent\nruncmd:\n- [ systemctl, start, qemu-guest-agent ]\n- name: \"sample-node-2\"\nimage:\ntype: \"ghcr.io/insrapperswil/network-ninja\"\nversion: \"latest\"\n- name: \"sample-node-3\"\nimage:\ntype: \"ubuntu\"\nversion: \"latest\"\nkind: \"pod\"\nconnections:\n- neighbors: \"TestHost1:1,TestHost2:1\"\nport: 22\n</code></pre>"},{"location":"concepts/#lab-instance","title":"Lab Instance","text":"<p>A lab instance is a CR that describes a lab that you want to deploy in a Kubernetes cluster. It has a reference to the lab template you want to use and also has a status field that is updated by the operator, which shows how many pods and VMs are running in the lab and the status of the lab instance itself.</p> <p>The following YAML is an example of a LabInstance:</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabInstance\nmetadata:\nlabels:\napp.kubernetes.io/name: labinstance\napp.kubernetes.io/instance: labinstance-sample\napp.kubernetes.io/part-of: operator\napp.kubernetes.io/managed-by: kustomize\napp.kubernetes.io/created-by: operator\nname: labinstance-sample\nspec:\nlabTemplateReference: \"labtemplate-sample\"\n</code></pre>"},{"location":"concepts/#lab","title":"Lab","text":"<p>A deployment of a lab instance is called a lab.</p>"},{"location":"concepts/#nodetype","title":"NodeType","text":"<p>In a network, a node represents any device that is part of the lab. A NodeType is a CR that defines a type of node that can be part of a lab. You reference the node type you want to have in your lab in the lab template. Within LTB, a node can be either a KubeVirt virtual machine or a pod.</p>"},{"location":"concepts/#network-topology","title":"Network Topology","text":"<p>The arrangement or pattern in which all nodes on a network are connected together is referred to as the network\u2019s topology.</p>"},{"location":"user-guide/","title":"User Guide","text":""},{"location":"user-guide/#installation-pre-requisites","title":"Installation (Pre-requisites)","text":"Tool Version Installation Description Kubernetes ^1.26.0 Installation Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubevirt 0.59.0 Installation Kubevirt is a Kubernetes add-on to run virtual machines on Kubernetes. Multus-CNI 3.9.0 Installation Multus-CNI is a plugin for K8s to attach multiple network interfaces to pods."},{"location":"user-guide/#example-lab-template","title":"Example Lab Template","text":"<p>This is an example of lab template, which you can use as a starting point for your own labs.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabTemplate\nmetadata:\nname: labtemplate-sample\nspec:\nnodes:\n- name: \"sample-node-1\"\nimage:\ntype: \"ubuntu\"\nversion: \"22.04\"\nkind: \"vm\"\nconfig: |-\n#cloud-config\npassword: ubuntu\nchpasswd: { expire: False }\nssh_authorized_keys:\n- &lt;your-ssh-pub-key&gt;\npackages:\n- qemu-guest-agent\nruncmd:\n- [ systemctl, start, qemu-guest-agent ]\n- name: \"sample-node-2\"\nimage:\ntype: \"ghcr.io/insrapperswil/network-ninja\"\nversion: \"latest\"\n- name: \"sample-node-3\"\nimage:\ntype: \"ubuntu\"\nversion: \"latest\"\nkind: \"pod\"\nconnections:\n- neighbors: \"TestHost1:1,TestHost2:1\"\n</code></pre> <p>The above lab template will define three nodes and one connection between two of the nodes.</p>"},{"location":"user-guide/#example-lab-instance","title":"Example Lab Instance","text":"<p>This is an example of lab instance, which you can use as a starting point for your own labs.</p> <pre><code>apiVersion: ltb-backend.ltb/v1alpha1\nkind: LabInstance\nmetadata:\nname: labinstance-sample\nspec:\nlabTemplateReference: \"labtemplate-sample\"\n</code></pre> <p>The above lab instance will create a lab instance called labinstance-sample using the data from the referenced resource labtemplate-sample, which is provided at the beginning as an example.</p>"},{"location":"architecture/k8s-ltb-architecture/","title":"Kubernetes Lab Topology Builder Architecture","text":"<p>The Kubernetes Lab Topology Builder (K8s-LTB) is a Kubernetes native version of the Lab Topology Builder (LTB). It is composed of a Frontend and a Backend.</p> <p></p>"},{"location":"architecture/k8s-ltb-architecture/#frontend","title":"Frontend","text":"<p>The frontend is responsible for the following tasks:</p> <ul> <li>Providing a web UI for the user to interact with the labs.</li> <li>Providing a web UI for the admin to manage:</li> <li>Users</li> <li>Lab templates</li> <li>Lab deployments</li> <li>Reservations</li> </ul>"},{"location":"architecture/k8s-ltb-architecture/#backend","title":"Backend","text":"<p>The backend is composed of the following components:</p> <ul> <li>Operator</li> <li>Operator API</li> </ul> <p>The backend is responsible for the following tasks:</p> <ul> <li>Parsing the yaml topology files</li> <li>Deploying/destroying the containers and vms</li> <li>Exposes status of lab instances</li> <li>Enables you to access the deployed containers and vms via different protocols</li> <li>Provides remote ssh capabilities</li> <li>Exposes information about a node (version, groups, etc.)</li> <li>Exposes node resource usage</li> <li>Provides remote Wireshark capture capabilities</li> <li>Managing reservations (create, delete, etc.)</li> <li>User management</li> <li>etc.</li> </ul>"},{"location":"architecture/k8s-ltb-architecture/#c4-model","title":"C4 Model","text":""},{"location":"architecture/k8s-ltb-architecture/#system-context-diagram","title":"System Context Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#container-diagram","title":"Container Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#component-diagram","title":"Component Diagram","text":""},{"location":"architecture/k8s-ltb-architecture/#legend","title":"Legend","text":"<ul> <li>Dark blue: represents Personas (User, Admin)</li> <li>Blue: represents Internal Components (Frontend Web UI, LTB K8s Backend)</li> <li>Light blue: represents Components which will be implemented in this project (LTB Operator, LTB Operator API)</li> <li>Dark gray: represents External Components (K8s, Keycloak)</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/","title":"Current (KVM/Docker)-base LTB Architecture","text":"<p>The following diagram shows the current KVM/Docker based LTB.</p> <p></p> <p>Currently the KVM/Docker based LTB is composed of the following containers:</p> <ul> <li>Frontend built with React</li> <li>Backend built with Django</li> <li>Deployment built with docker-compose</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#backend","title":"Backend","text":"<p>The backend is accessible via API and a Admin Web UI. It is responsible for the following tasks:</p> <ul> <li>parsing the yaml topology files</li> <li>deploying/destroying the containers and vms</li> <li>exposes status of lab deployments</li> <li>exposes information on how to access the deployed containers and vms</li> <li>provides remote ssh capabilities</li> <li>provides remote Wireshark capture capabilities</li> <li>managing reservations (create, delete, etc.)</li> <li>exposes node resource usage</li> <li>user management</li> <li>exposes information about a device (version, groups, etc.)</li> </ul> <p>It is composed of the following components:</p> <ul> <li>Reservations</li> <li>Running lab store</li> <li>Template store</li> <li>Authentication</li> </ul> <p>The orchestration component is responsible for creating different tasks using Celery and executing them on a remote host. There are 4 different types of tasks:</p> <ul> <li>DeploymentTask</li> <li>Deploys containers in docker</li> <li>Deploys VMs using KVM</li> <li>Creates connections between containers and VMs using an OVS bridge</li> <li>RemovalTask</li> <li>Removes a running lab</li> <li>MirrorInterfaceTask</li> <li>Creates a mirror interface on a connection</li> <li>SnapshotTask</li> <li>Takes a snapshot of a running lab</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#reservations","title":"Reservations","text":"<p>The reservation component is responsible for reserving system resources in advance. It is responsible for the following tasks:</p> <ul> <li>Create a reservation</li> <li>Delete a reservation</li> <li>Update a reservation</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#running-lab-store","title":"Running lab store","text":"<p>This component is responsible for storing information about running labs, such as:</p> <ul> <li>The devices taking part in the running lab, inclusive of the interfaces</li> <li>Connection information</li> </ul>"},{"location":"architecture/kvm-docker-ltb-architecture/#template-store","title":"Template store","text":"<p>This component is responsible for storing lab templates.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#authentication","title":"Authentication","text":"<p>This component is responsible for user authentication and management.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#deployment","title":"Deployment","text":"<p>The deployment component is responsible for deploying the LTB backend and frontend components.</p>"},{"location":"architecture/kvm-docker-ltb-architecture/#frontend","title":"Frontend","text":"<p>The frontend allows users to access their labs and their devices.</p>"},{"location":"contributor/coding-conventions/","title":"Coding Conventions","text":""},{"location":"contributor/coding-conventions/#naming","title":"Naming","text":"<p>The following naming conventions are used in the project:</p>"},{"location":"contributor/coding-conventions/#naming-conventions-in-go","title":"Naming conventions in Go","text":"<ul> <li>camelCase for variables and functions, which are not exported</li> <li>PascalCase for types and functions that need to be exported</li> </ul>"},{"location":"contributor/coding-conventions/#examples","title":"Examples","text":"<ul> <li>labInstanceStatus: variable name for a status of a lab instance</li> <li>UpdateLabInstanceStatus: name for an exported function, starts with a capital letter</li> </ul>"},{"location":"contributor/coding-conventions/#coding","title":"Coding","text":"<ul> <li>The Go extension in VSCode has a linting capability, so that will be used for linting and formatting.</li> </ul>"},{"location":"contributor/dev-cluster-setup/","title":"Development cluster setup","text":"<p>Setup for a development Kubernetes cluster.</p>"},{"location":"contributor/dev-cluster-setup/#prepare-node","title":"Prepare Node","text":"<pre><code>sudo apt update\nsudo apt upgrade -y\nsudo swapoff -a\nsudo sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#rke2-server-configuration","title":"RKE2 Server Configuration","text":"<pre><code>sudo mkdir -p /etc/rancher/rke2\nsudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <pre><code># /etc/rancher/rke2/config.yaml\nwrite-kubeconfig-mode: \"0644\"\nkube-apiserver-arg: \"allow-privileged=true\"\ncni: multus,cilium\ndisable-kube-proxy: true\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#cilium-configuration-for-multus","title":"Cilium Configuration for Multus","text":"<pre><code>sudo mkdir -p /var/lib/rancher/rke2/server/manifests\nsudo vim /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml\n</code></pre> <pre><code># /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml\n# k8sServiceHost/Port IP of Control Plane node default Port 6443\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\nname: rke2-cilium\nnamespace: kube-system\nspec:\nvaluesContent: |-\ncni:\nchainingMode: \"none\"\nexclusive: false\nkubeProxyReplacement: strict\nk8sServiceHost: \"&lt;NodeIP&gt;\"\nk8sServicePort: 6443\noperator:\nreplicas: 1\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#install-and-start-server-and-check-logs","title":"Install and start Server and check logs","text":"<pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=v1.26.0+rke2r2 sudo -E sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\nsudo journalctl -u rke2-server -f\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#add-kubernetes-tools-to-path-and-set-kubeconfig","title":"Add Kubernetes tools to path and set kubeconfig","text":"<p>Adds kubectl, crictl and ctr to path</p> <pre><code>echo 'export PATH=\"$PATH:/var/lib/rancher/rke2/bin\"' &gt;&gt; ~/.bashrc\necho 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bashrc\necho 'alias k=kubectl' &gt;&gt; ~/.bashrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt;~/.bashrc\nsource ~/.bashrc\nmkdir ~/.kube\nln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#get-token-for-agent","title":"Get Token for Agent","text":"<pre><code>sudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#rke2-agent-configuration-optional","title":"RKE2 Agent Configuration (Optional)","text":"<pre><code>sudo mkdir -p /etc/rancher/rke2\nsudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <pre><code># /etc/rancher/rke2/config.yaml\n---\nserver: https://&lt;server&gt;:9345\ntoken: &lt;token from server node&gt;\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#install-and-start-agent-and-check-logs","title":"Install and start Agent and check logs","text":"<pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" INSTALL_RKE2_VERSION=v1.26.0+rke2r2 sudo -E sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\nsudo journalctl -u rke2-agent -f\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#install-cluster-network-addons-operator","title":"Install Cluster Network Addons Operator","text":"<p>The Cluster Network Addons Operator can be used to deploy additional networking components. Multus and Cilium are already installed via RKE2. Open vSwitch CNI Plugin can be installed via this operator.</p> <p>First install the operator itself:</p> <pre><code>kubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/namespace.yaml\nkubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/network-addons-config.crd.yaml\nkubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/operator.yaml\n</code></pre> <p>Then you need to create a configuration for the operator example CR:</p> <pre><code>kubectl apply -f https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.85.0/network-addons-config-example.cr.yaml\n</code></pre> <p>Wait until the operator has finished the installation:</p> <pre><code>kubectl wait networkaddonsconfig cluster --for condition=Available\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#kubevirt","title":"Kubevirt","text":"<p>Kubevirt is a Kubernetes add-on to run virtual machines.</p>"},{"location":"contributor/dev-cluster-setup/#validate-hardware-virtualization-support","title":"Validate Hardware Virtualization Support","text":"<pre><code>sudo apt install libvirt-clients\nsudo virt-host-validate qemu\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#install-kubevirt","title":"Install Kubevirt","text":"<p>Latest Release: <code>export RELEASE=$(curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt)</code></p> <pre><code>export RELEASE=v0.58.1\n# Deploy the KubeVirt operator\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml\n# Create the KubeVirt CR (instance deployment request) which triggers the actual installation\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml\n# wait until all KubeVirt components are up\nkubectl -n kubevirt wait kv kubevirt --for condition=Available\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#install-containerized-data-importer","title":"Install Containerized Data Importer","text":"<pre><code>export CDI_VERSION=v1.55.2\nkubectl create ns cdi\nkubectl -n cdi apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml\nkubectl -n cdi apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#install-virtctl-via-krew","title":"Install virtctl via Krew","text":"<p>First install Krew and then install virtctl via Krew</p> <pre><code>(\nset -x; cd \"$(mktemp -d)\" &amp;&amp;\nOS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\nARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\nKREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\ncurl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\ntar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n./\"${KREW}\" install krew\n)\necho 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nkubectl krew install virt\nkubectl virt help\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#metallb","title":"MetalLB","text":"<p>MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.</p>"},{"location":"contributor/dev-cluster-setup/#install-operator-lifecycle-manager-olm","title":"Install Operator Lifecycle Manager (OLM)","text":"<p>Install Operator Lifecycle Manager (OLM), a tool to help manage the Operators running on your cluster.</p> <pre><code>curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.24.0/install.sh | bash -s v0.24.0\n</code></pre> <p>Install the operator by running the following command:</p> <pre><code>kubectl create -f https://operatorhub.io/install/metallb-operator.yaml\n</code></pre> <p>This Operator will be installed in the \"operators\" namespace and will be usable from all namespaces in the cluster.</p> <p>After install, watch your operator come up using next command.</p> <pre><code>kubectl get csv -n operators\n</code></pre> <p>Now create a MetalLB IPAddressPool CR to configure the IP address range that MetalLB will use:</p> <pre><code>sudo vim metallb-ipaddresspool.yaml\n</code></pre> <pre><code># metallb-ipaddresspool.yaml\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\nname: default\nnamespace: operators\nspec:\naddresses:\n- X.X.X.X/XX\n</code></pre> <p>Create a L2Advertisement to tell MetalLB to responde to ARP requests for all IP address pools (no named ip address pool, means all pools):</p> <pre><code>sudo vim l2advertisment.yaml\n</code></pre> <pre><code># l2advertisment.yaml\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\nname: default\nnamespace: operators\nspec:\nipAddressPools:\n- default\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f metallb-ipaddresspool.yaml\nkubectl apply -f l2advertisment.yaml\n</code></pre>"},{"location":"contributor/dev-cluster-setup/#trident","title":"Trident","text":"<p>Check connectivity to NetApp Storage:</p> <pre><code>kubectl run -i --tty ping --image=busybox --restart=Never --rm -- \\\nping &lt;NetApp Management IP&gt;\n</code></pre> <p>Download and extract the Trident installer:</p> <pre><code>export TRIDENT_VERSION=23.01.0\nwget https://github.com/NetApp/trident/releases/download/v$TRIDENT_VERSION/trident-installer-$TRIDENT_VERSION.tar.gz\ntar -xf trident-installer-$TRIDENT_VERSION.tar.gz\ncd trident-installer\nmkdir setup\nvim ./setup/backend.json\n</code></pre> <p>Configure the installer:</p> <pre><code># ./backend.json\n</code></pre>"},{"location":"contributor/technologies-used/","title":"Tools and Frameworks","text":"<p>The tools and frameworks used in the project are listed below.</p>"},{"location":"contributor/technologies-used/#go-based-operator-sdk-framework","title":"Go-based Operator SDK framework","text":"<p>To create the LTB operator, we used the Go-based Operator-sdk framework, which provides a set of tools to simplify the process of building, testing and packaging our operator.</p>"},{"location":"contributor/technologies-used/#kubevirt","title":"Kubevirt","text":"<p>Kubevirt is a tool that provides a virtual machine management layer on top of Kubernetes. It allows us to deploy virtual machines on Kubernetes.</p>"},{"location":"contributor/technologies-used/#kubernetes","title":"Kubernetes","text":"<p>We use Kubernetes as the container orchestration platform for the LTB application.</p>"},{"location":"contributor/technologies-used/#multus-cni","title":"Multus CNI","text":"<p>To create multiple network interfaces for the pods, Multus CNI is used.</p>"},{"location":"contributor/test-concepts/","title":"Test Concept","text":""},{"location":"contributor/test-concepts/#overview","title":"Overview","text":"<p>This document outlines the approaches, methodologies, and types of tests that will be used to ensure the LTB K8s Backend components are functioning as expected.</p>"},{"location":"contributor/test-concepts/#test-categories","title":"Test categories","text":"<p>The tests will primarily focus on the following category:</p> <ul> <li>Functionality and Logic: This includes automated integration tests to evaluate how the LTB K8s Backend interacts with other components of the LTB application, such as the operator's function in a Kubernetes cluster with a K8s API server and other resources.</li> </ul> <p>Testing in the other categories, such as Security and Performance, will be considered later time and their specifics will be determined accordingly.</p>"},{"location":"contributor/test-concepts/#tools","title":"Tools","text":"<p>The tools listed below are going to be used to perform the tests mentioned above. Moreover, the tools are used in a suite test, which is created when a controller is scaffolded by the tool.</p> <ul> <li>Testify: a go package that provides a set of features to perform unit tests, such as assertions, mocks, etc.</li> <li>EnvTest: a Go library that helps write integration tests for Kubernetes controllers by setting up an instance of etcd and a Kubernetes API server, without kubelet, controller-manager, or other components.</li> <li>Ginkgo: a Go testing framework for Go to help you write epxressive, readable, and maintainable tests. It is best used with the Gomega matcher library.</li> <li>Gomega: a Go matcher library that provides a set of matchers to perform assertions in tests. It is best used with the Ginkgo testing framework.</li> </ul>"},{"location":"contributor/test-concepts/#strategies-test-approach","title":"Strategies: Test Approach","text":"<p>The following test approaches are going to be used to test the LTB K8s Backend components:</p>"},{"location":"contributor/test-concepts/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are going to be used to test small pieces of code, such as functions, which don't involve setting up testing Kubernetes environment with a K8s API server and other resources.</p>"},{"location":"contributor/test-concepts/#integration-tests","title":"Integration Tests","text":"<p>Integration tests are going to be used to test the different components of the LTB K8s Backend, such as the operator, the controllers, etc., and how they interact with each other.</p>"},{"location":"contributor/test-concepts/#environment","title":"Environment","text":"<p>EnvTest is going to be used to set up a testing Kubernetes environment with a K8s API server and other resources.</p>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/","title":"Use Markdown Architectural Decision Records","text":""},{"location":"decisions/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record design decisions made in this project. Which format and structure should these records follow?</p>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR 2.1.2 \u2013 The Markdown Architectural Decision Records</li> <li>Michael Nygard's template \u2013 The first incarnation of the term \"ADR\"</li> <li>Sustainable Architectural Decisions \u2013 The Y-Statements</li> <li>Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"decisions/0000-use-markdown-architectural-decision-records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 2.1.2\", because</p> <ul> <li>Implicit assumptions should be made explicit.   Design documentation is important to enable people understanding the decisions later on.   See also A rational design process: How and why to fake it.</li> <li>The MADR format is lean and fits our development style.</li> <li>The MADR structure is comprehensible and facilitates usage &amp; maintenance.</li> <li>The MADR project is vivid.</li> <li>Version 2.1.2 is the latest one available when starting to document ADRs.</li> <li>A Visual Studio Code extension ADR Manager for MADR exits, which makes managing ADRs easy.</li> </ul>"},{"location":"decisions/0001-operator-SDK/","title":"Operator SDK","text":""},{"location":"decisions/0001-operator-SDK/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>It's best practice to use an SDK to build operators for Kubernetes. The SDK provides a higher level of abstraction for creating Kubernetes operators, which makes it easier to write and manage operators. There are multiple SDKs available for building operators. We need a SDK that's flexible and easy to use and can be used with Go.</p>"},{"location":"decisions/0001-operator-SDK/#considered-options","title":"Considered Options","text":"<ul> <li>Operator SDK (Operator Framework)</li> <li>KubeBuilder</li> <li>Kopf</li> <li>KUDO</li> <li>Metacontroller</li> </ul>"},{"location":"decisions/0001-operator-SDK/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Operator SDK\", because it provides a high level of abstraction for creating Kubernetes operators, which makes it easier to write and manage operators. Additionally, there are tools and libraries for building and testing the operator included in Operator SDK, it's easy to use and can be used with Go.</p>"},{"location":"decisions/0001-operator-SDK/#more-information","title":"More Information","text":"<ul> <li>Operator SDK</li> <li>Tools to build an operator</li> </ul>"},{"location":"decisions/00011-remote-access/","title":"Remote-Access","text":""},{"location":"decisions/00011-remote-access/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>For the labinstances to be usefull for the students, they need to be able to access the pods (containers) and VMs. The access should be restricted to the pods/VMs the user is allowed to access. It should be possible to access the pods/VMs console and or access it via multiple OOB protocols (SSH, RDP, VNC, ...).</p>"},{"location":"decisions/00011-remote-access/#considered-options","title":"Considered Options","text":"<ul> <li>Kubernetes Service</li> <li>Gotty</li> <li>ttyd</li> </ul>"},{"location":"decisions/00011-remote-access/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"ttyd and Kubernetes Service\", ttyd will be used as a jump host to access the pods/VMs console, and a Kubernetes service (LoadBalancer) will be used to access the pods/VMs via OOB protocols. Security for the console access will likely be easy to implement. Secure access via OOB protocols was considered, but will need to be reasearched further.</p>"},{"location":"decisions/0002-operator-scope/","title":"Operator Scope","text":""},{"location":"decisions/0002-operator-scope/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The Operator could be a namespace-scoped or cluster-scoped.</p>"},{"location":"decisions/0002-operator-scope/#considered-options","title":"Considered Options","text":"<ul> <li>Namespace-scoped</li> <li>Cluster-scoped</li> </ul>"},{"location":"decisions/0002-operator-scope/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Cluster-scoped\", because cluster-scoped operators enables you to manage namespaces or resources in the entire cluster. They are also capable of managing infrastructure-level resources, such as nodes. Additionally, cluster-scoped operators provide us greater visibility and control over the entire cluster.</p>"},{"location":"decisions/0002-operator-scope/#more-information","title":"More Information","text":"<ul> <li>Operator Scope</li> </ul>"},{"location":"decisions/0003-api-and-operator-run-in-one-container/","title":"API and Operator Run in One Container","text":""},{"location":"decisions/0003-api-and-operator-run-in-one-container/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The operator and the API could be separated and deployed as two services/containers.</p>"},{"location":"decisions/0003-api-and-operator-run-in-one-container/#considered-options","title":"Considered Options","text":"<ul> <li>One container</li> <li>Separate containers</li> </ul>"},{"location":"decisions/0003-api-and-operator-run-in-one-container/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"One container\", because it simplifies the deployment and implementation as everything will be written in one programming language, with the drawback that the components can't be exchanged easily.</p>"},{"location":"decisions/0004-dev-container/","title":"DevContainer","text":""},{"location":"decisions/0004-dev-container/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Every team member could set up their development environment manually or they can create an automated and same development environment for everyone by using a DevContainer.</p>"},{"location":"decisions/0004-dev-container/#considered-options","title":"Considered Options","text":"<ul> <li>DevContainer</li> <li>Manual Setup</li> </ul>"},{"location":"decisions/0004-dev-container/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"DevContainer\", because DevContainer setup lets you create the same development environment for all team members, which ensures consistency. It also provides a completely isolated development environment, which helps to avoid software incompatibility issues, such as operator-sdk not working on Windows. Moreover, DevContainer is easily portable and can be shared between team members irrespective of the operating system they use.</p>"},{"location":"decisions/0004-dev-container/#more-information","title":"More Information","text":"<ul> <li>DevContainer</li> </ul>"},{"location":"decisions/0005-replace-current-ltb-backend/","title":"Replace Current LTB Backend","text":""},{"location":"decisions/0005-replace-current-ltb-backend/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The LTB K8s Backend could replace the current LTB Backend fully or partially by leaving features, such as User-Management in the current LTB Backend.</p>"},{"location":"decisions/0005-replace-current-ltb-backend/#considered-options","title":"Considered Options","text":"<ul> <li>Replace current LTB Backend fully</li> <li>Replace current LTB Backend partially</li> </ul>"},{"location":"decisions/0005-replace-current-ltb-backend/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Replace current LTB Backend fully\", because huge parts of the current LTB Backend would need to be rewritten to be compatible with the new LTB K8s operator.</p>"},{"location":"decisions/0006-lab-instance-set/","title":"Lab Instance Set","text":""},{"location":"decisions/0006-lab-instance-set/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We could use one CR, LabInstanceSet, and tell the operator we want e.g. 10 LabInstances  by only providing one LabInstance CR or we could provide the operator 10 CRs to create 10 LabInstances.</p>"},{"location":"decisions/0006-lab-instance-set/#considered-options","title":"Considered Options","text":"<ul> <li>With LabInstanceSet</li> <li>Without LabInstanceSet</li> </ul>"},{"location":"decisions/0006-lab-instance-set/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"With LabInstanceSet\", because it is easier to manage, and helps to avoid redundancy...</p>"},{"location":"decisions/0007-deployment-to-k8s/","title":"Deployment to K8s","text":""},{"location":"decisions/0007-deployment-to-k8s/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Files could be stored in git and deployed to a K8s cluster from there or we could use the frontend for that case.</p>"},{"location":"decisions/0007-deployment-to-k8s/#considered-options","title":"Considered Options","text":"<ul> <li>Store files in Git</li> <li>Use frontend</li> </ul>"},{"location":"decisions/0007-deployment-to-k8s/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Store files in Git\", because it makes automation of the deployment easier and reduces the possibility of human errors.</p>"},{"location":"decisions/0008-namespace-per-labinstance/","title":"Namespace Per LabInstance","text":""},{"location":"decisions/0008-namespace-per-labinstance/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>LabInstance could be created in separate namespaces or one namespace for all LabInstances.</p>"},{"location":"decisions/0008-namespace-per-labinstance/#considered-options","title":"Considered Options","text":"<ul> <li>One Namespace for all LabInstances</li> <li>Namespace per LabInstance</li> </ul>"},{"location":"decisions/0008-namespace-per-labinstance/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Namespace per LabInstance\", because not decided yet.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/","title":"K8s Aggreted API Over Standalone API","text":""},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want a declarative way of creating LTB labs inside Kubernetes using Kubernetes native pods and KubeVirt virtual machines.</p>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#considered-options","title":"Considered Options","text":"<ul> <li>Standalone API</li> <li>Aggregated API</li> </ul>"},{"location":"decisions/0009-k8s-aggreted-api-over-standalone-api/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Aggregated API\", because Is better suited for declerative API, our new types will be readable and writeable using kubectl/Kubernetes tools, such as dashboards. We also can leverage Kuberbetes API support features this way. Our resources are scoped to a cluster or namespaces of a cluster.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/","title":"Extending LTB with New Node Types","text":"<ul> <li>Status: Proposed</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>It should be possible to create, update and delete node types (e.g. Ubuntu, XRD, XR, IOS, Cumulus etc.) Node types should be used inside lab templates and expose a way to provide a node with configuration (cloud-init, zero-touch, etc.) The amount of available network interfaces is dynamic and depends on how many connections a node has according to a specific lab template.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Certain operating systems' images like XR, and XRD need a specific interface configuration which depends on how many interfaces a certain node will receive.</li> <li>The chosen solution should support multiple version of a type in an easy to use way (e.g. Ubuntu 22.04, 20.04, ...).</li> <li>For XRd images, interfaces need to have environment variables set for each interface they use, and the interface count needs to be dynamically set according to the lab template.</li> <li>For XR VM images, the first interface is the management interface and then there are two empty interfaces need a special configuration.</li> <li>For mount from config might be different</li> <li>Cumulus VX images need a privileged container</li> <li>XRd need additional privileges</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#considered-options","title":"Considered Options","text":"<ul> <li>Custom Resources</li> <li>Go</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Custom Resources\", because it will be possible to support all the cases mentioned in the decision drivers using go templates and a CRs. Implementing the types in Go does not seem to bring any major advantages, whereas using CRs will be easier for external users to extend the system with new node types.</p>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Easy to extend during runtime</li> <li>Easy to extend for external users</li> <li>All decision drivers will be supported</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Go Templates are not as powerful as Go, which could make it harder.</li> </ul>"},{"location":"decisions/0010-extending-ltb-with-new-node-types/#links","title":"Links","text":"<ul> <li>https://github.com/vrnetlab/vrnetlab</li> </ul>"}]}